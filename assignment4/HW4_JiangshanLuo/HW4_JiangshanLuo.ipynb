{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Constrainted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Assignment 4: NER\n",
    "# This is just to help you get going. Feel free to\n",
    "# add to or modify any part of it.\n",
    "\n",
    "\n",
    "def getfeats(word, o):\n",
    "    \"\"\" This takes the word in question and\n",
    "    the offset with respect to the instance\n",
    "    word \"\"\"\n",
    "    o = str(o)\n",
    "    features = [\n",
    "        (o + 'word', word),\n",
    "        # TODO: add more features here.\n",
    "        ('contain_digit', any(char.isdigit() for char in word)),\n",
    "        ('all_digit', all(char.isdigit() for char in word)),\n",
    "        ('contain_cap', any(char.isupper() for char in word)),\n",
    "        ('all_cap', all(char.isupper() for char in word)),\n",
    "        ('contain_specChar', any(not char.isalpha() and not char.isdigit() for char in word))\n",
    "    ]\n",
    "    #print(features)\n",
    "    return features\n",
    "    \n",
    "\n",
    "def word2features(sent, i):\n",
    "    \"\"\" The function generates all features\n",
    "    for the word at position i in the\n",
    "    sentence.\"\"\"\n",
    "    features = []\n",
    "    # the window around the token\n",
    "    for o in range(-1, 2):\n",
    "        if i+o >= 0 and i+o < len(sent):\n",
    "            word = sent[i+o][0]\n",
    "            featlist = getfeats(word, o)\n",
    "            features.extend(featlist)\n",
    "    \n",
    "    return dict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22438339\n",
      "Iteration 2, loss = 0.07631547\n",
      "Iteration 3, loss = 0.05247261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to results.txt\n",
      "Now run: python conlleval.py constrained_results.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the training data\n",
    "    train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "    dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "    test_sents = list(conll2002.iob_sents('esp.testb'))\n",
    "    \n",
    "    train_feats = []\n",
    "    train_labels = []\n",
    "\n",
    "    for sent in train_sents:\n",
    "        for i in range(len(sent)):\n",
    "            feats = word2features(sent,i)\n",
    "            train_feats.append(feats)\n",
    "            train_labels.append(sent[i][-1])\n",
    "\n",
    "    vectorizer = DictVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_feats)\n",
    "\n",
    "    # TODO: play with other models\n",
    "    #model = Perceptron(verbose=1)\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    model = MLPClassifier(learning_rate_init=0.005, verbose=1, max_iter=3)\n",
    "    \n",
    "#     from sklearn.tree import DecisionTreeClassifier\n",
    "#     model = DecisionTreeClassifier(criterion='entropy', verbose=1)\n",
    "    \n",
    "#     from sklearn.ensemble import RandomForestClassifier\n",
    "#     model = RandomForestClassifier(verbose=1)\n",
    "    \n",
    "    model.fit(X_train, train_labels)\n",
    "\n",
    "    test_feats = []\n",
    "    test_labels = []\n",
    "\n",
    "    # switch to test_sents for your final results\n",
    "    for sent in dev_sents:\n",
    "        for i in range(len(sent)):\n",
    "            feats = word2features(sent,i)\n",
    "            test_feats.append(feats)\n",
    "            test_labels.append(sent[i][-1])\n",
    "\n",
    "    X_test = vectorizer.transform(test_feats)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    j = 0\n",
    "    print(\"Writing to results.txt\")\n",
    "    # format is: word gold pred\n",
    "    with open(\"constrained_results.txt\", \"w\") as out:\n",
    "        for sent in dev_sents: \n",
    "            for i in range(len(sent)):\n",
    "                word = sent[i][0]\n",
    "                gold = sent[i][-1]\n",
    "                pred = y_pred[j]\n",
    "                j += 1\n",
    "                out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
    "        out.write(\"\\n\")\n",
    "\n",
    "    print(\"Now run: python conlleval.py constrained_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 52923 tokens with 4351 phrases; found: 4930 phrases; correct: 2581.\n",
      "accuracy:  93.61%; precision:  52.35%; recall:  59.32%; FB1:  55.62\n",
      "              LOC: precision:  47.70%; recall:  75.91%; FB1:  58.59  1566\n",
      "             MISC: precision:  26.58%; recall:  32.13%; FB1:  29.09  538\n",
      "              ORG: precision:  58.62%; recall:  52.59%; FB1:  55.44  1525\n",
      "              PER: precision:  61.26%; recall:  65.22%; FB1:  63.18  1301\n"
     ]
    }
   ],
   "source": [
    "import conlleval\n",
    "\n",
    "conlleval.main([\"\", \"constrained_results.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Unconstrainted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from nltk.corpus import conll2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use google pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.17675781e-01 -2.34375000e-01  4.76562500e-01 -1.53320312e-01\n",
      "  5.03906250e-01  1.07910156e-01  2.79296875e-01  9.91210938e-02\n",
      "  3.14941406e-02  1.82617188e-01  2.08007812e-01 -9.91210938e-02\n",
      " -1.90429688e-01  1.59179688e-01 -3.28063965e-03  2.21679688e-01\n",
      "  3.10546875e-01  1.42578125e-01 -8.05664062e-02 -4.51171875e-01\n",
      "  1.01928711e-02  6.25000000e-02 -1.04003906e-01  2.17285156e-02\n",
      "  3.51562500e-01 -5.97656250e-01  6.03027344e-02  3.37219238e-03\n",
      "  3.06396484e-02  4.76562500e-01 -1.02539062e-01 -2.05078125e-01\n",
      " -2.53906250e-01  2.30468750e-01  1.36718750e-01  3.47656250e-01\n",
      "  1.05957031e-01 -2.22167969e-02 -3.32031250e-01  4.35546875e-01\n",
      " -1.44531250e-01  4.73632812e-02 -9.64355469e-03 -2.10937500e-01\n",
      "  2.16064453e-02 -6.98242188e-02 -2.47802734e-02  7.12890625e-02\n",
      " -3.20312500e-01  3.02734375e-01  1.03515625e-01 -1.25976562e-01\n",
      " -1.95312500e-01  6.05468750e-02  8.93554688e-02 -3.82812500e-01\n",
      "  7.08007812e-02  1.13525391e-02 -2.98828125e-01 -2.50000000e-01\n",
      "  4.22363281e-02 -1.89453125e-01  1.57226562e-01 -8.00781250e-02\n",
      "  1.40625000e-01  2.29492188e-01  1.18164062e-01 -3.51562500e-01\n",
      " -4.64843750e-01 -1.81640625e-01  4.23828125e-01 -6.05468750e-02\n",
      " -1.46484375e-01 -2.32421875e-01 -2.61718750e-01 -9.66796875e-02\n",
      " -4.93164062e-02  3.16406250e-01  4.23828125e-01 -1.20117188e-01\n",
      "  1.04980469e-02 -2.96875000e-01 -7.22656250e-02  1.00097656e-01\n",
      " -1.57226562e-01 -3.24707031e-02 -1.40625000e-01  4.33593750e-01\n",
      "  3.16406250e-01  7.47070312e-02  3.00781250e-01 -2.31445312e-01\n",
      "  8.34960938e-02  7.81250000e-02 -1.00097656e-01 -3.18359375e-01\n",
      " -1.73339844e-02 -4.16015625e-01 -1.66015625e-01 -2.61718750e-01\n",
      " -1.74804688e-01 -7.12890625e-02  3.46679688e-02 -1.64062500e-01\n",
      "  2.21679688e-01  1.03759766e-02 -1.79687500e-01  8.44726562e-02\n",
      "  1.20117188e-01 -2.34375000e-01 -1.50390625e-01 -3.43750000e-01\n",
      " -6.93359375e-02  9.76562500e-02 -1.42578125e-01 -3.78906250e-01\n",
      "  2.07519531e-02  1.63574219e-02  1.76757812e-01  1.09863281e-01\n",
      "  4.63867188e-02  9.47265625e-02 -5.31250000e-01  1.86523438e-01\n",
      "  1.44531250e-01 -5.03906250e-01  1.84570312e-01 -2.12890625e-01\n",
      "  8.20312500e-02 -1.62109375e-01  2.50000000e-01  6.20117188e-02\n",
      " -2.43164062e-01 -2.57812500e-01 -4.27734375e-01  1.52343750e-01\n",
      "  1.11816406e-01  2.59765625e-01  2.69531250e-01  1.00585938e-01\n",
      " -1.44195557e-03  1.87500000e-01 -2.08984375e-01 -1.98364258e-03\n",
      " -1.14746094e-01  7.76367188e-02 -1.62109375e-01 -1.25000000e-01\n",
      " -8.20312500e-02 -3.47656250e-01  4.72656250e-01 -2.57812500e-01\n",
      " -7.81250000e-03  1.56250000e-01  7.66601562e-02 -3.32031250e-01\n",
      " -9.22851562e-02  1.25976562e-01 -5.88378906e-02 -3.16406250e-01\n",
      " -9.27734375e-02  3.39843750e-01  3.86718750e-01  2.33398438e-01\n",
      " -4.94140625e-01  8.60595703e-03  2.34375000e-01  2.65502930e-03\n",
      "  5.90820312e-02  5.03906250e-01  4.39453125e-01 -2.22656250e-01\n",
      " -2.55859375e-01 -4.19921875e-02 -1.01074219e-01  5.61523438e-02\n",
      "  1.26953125e-01  2.50000000e-01 -1.62353516e-02 -3.32031250e-01\n",
      " -9.96093750e-02 -3.17382812e-02 -1.71875000e-01 -2.59765625e-01\n",
      "  2.94921875e-01  1.98974609e-02  1.97265625e-01  3.90625000e-02\n",
      "  1.21093750e-01  1.05468750e-01 -2.63671875e-01  1.81640625e-01\n",
      " -1.71875000e-01 -2.27539062e-01 -7.71484375e-02  1.51367188e-01\n",
      " -1.35742188e-01 -8.88671875e-02 -1.25976562e-01 -3.24707031e-02\n",
      "  3.36914062e-02  6.83593750e-02 -1.24023438e-01 -1.28906250e-01\n",
      "  1.96289062e-01  7.71484375e-02 -2.81250000e-01 -1.57226562e-01\n",
      " -4.85839844e-02 -2.07031250e-01 -1.81640625e-01 -5.44433594e-02\n",
      "  3.33984375e-01 -5.41992188e-02 -9.76562500e-02  2.21679688e-01\n",
      " -2.89062500e-01 -7.32421875e-02 -1.64062500e-01 -2.85156250e-01\n",
      "  5.85937500e-01  1.62109375e-01 -7.95898438e-02  1.69921875e-01\n",
      "  3.59375000e-01 -4.80468750e-01 -2.50244141e-03  1.51977539e-02\n",
      " -1.41601562e-01  2.00195312e-01 -7.61718750e-02 -5.00000000e-01\n",
      "  1.25976562e-01 -3.61328125e-01 -9.47265625e-02  1.43554688e-01\n",
      " -3.75000000e-01  4.29687500e-01  2.39257812e-01  9.47265625e-02\n",
      " -6.68945312e-02  7.51953125e-02  7.71484375e-02 -7.50000000e-01\n",
      " -1.88476562e-01  1.26953125e-01 -5.78613281e-02  2.91015625e-01\n",
      "  1.68945312e-01 -3.36914062e-02  1.11816406e-01  2.69531250e-01\n",
      " -1.03515625e-01  1.53320312e-01  2.71484375e-01 -1.16210938e-01\n",
      "  1.26953125e-01  1.62109375e-01  9.03320312e-02  2.44140625e-02\n",
      "  8.05664062e-02  1.25976562e-01 -3.35937500e-01 -1.86523438e-01\n",
      "  8.05664062e-02  1.49536133e-02 -1.82617188e-01  1.21093750e-01\n",
      " -1.64062500e-01  5.24902344e-02  1.70898438e-01 -1.85966492e-04\n",
      " -1.14746094e-01 -4.73632812e-02  1.93359375e-01  2.42187500e-01\n",
      " -2.50000000e-01 -1.11328125e-01  8.48388672e-03  1.21582031e-01\n",
      "  1.85546875e-01  1.16210938e-01 -3.97949219e-02 -7.03125000e-02\n",
      " -4.35546875e-01  1.99218750e-01  1.67968750e-01  4.95605469e-02\n",
      " -3.63281250e-01  2.09960938e-01 -4.14062500e-01 -1.43554688e-01\n",
      " -8.39843750e-02 -4.98046875e-01  7.66601562e-02  4.60815430e-03\n",
      "  7.86132812e-02 -2.45117188e-01  1.48437500e-01  7.71484375e-02]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# train model\n",
    "Word2Vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "print(Word2Vec_model['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfeats(word, o):\n",
    "    \"\"\" This takes the word in question and\n",
    "    the offset with respect to the instance\n",
    "    word \"\"\"\n",
    "    o = str(o)\n",
    "#     features = [\n",
    "#         (o + 'word', word),\n",
    "#         # TODO: add more features here.\n",
    "#         ('contain_digit', any(char.isdigit() for char in word)),\n",
    "#         ('all_digit', all(char.isdigit() for char in word)),\n",
    "#         ('contain_cap', any(char.isupper() for char in word)),\n",
    "#         ('all_cap', all(char.isupper() for char in word)),\n",
    "#         ('contain_specChar', any(not char.isalpha() and not char.isdigit() for char in word))\n",
    "#     ]\n",
    "    try:\n",
    "        word_vec = Word2Vec_model[word]\n",
    "    except:\n",
    "        word_vec = Word2Vec_model['unknown']\n",
    "    \n",
    "    features = list(word_vec) + [any(char.isdigit() for char in word), all(char.isdigit() for char in word),\n",
    "                any(char.isupper() for char in word), all(char.isupper() for char in word),\n",
    "                any(not char.isalpha() and not char.isdigit() for char in word)]\n",
    "    #print(features)\n",
    "    return features\n",
    "    \n",
    "\n",
    "def word2features(sent, i):\n",
    "    \"\"\" The function generates all features\n",
    "    for the word at position i in the\n",
    "    sentence.\"\"\"\n",
    "    features = []\n",
    "    # the window around the token\n",
    "    for o in range(-1, 2):\n",
    "        if i+o >= 0 and i+o < len(sent):\n",
    "            word = sent[i+o][0]\n",
    "        elif i+o < 0:\n",
    "            word = '$'\n",
    "        elif i+o >= len(sent):\n",
    "            word = '#'\n",
    "        featlist = getfeats(word, o)\n",
    "        features.extend(featlist)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "training_epoch = 1\n",
    "learning_rate = 0.005\n",
    "batch_size = 100\n",
    "\n",
    "# Network parameters\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 300\n",
    "n_hidden_3 = 300\n",
    "\n",
    "\n",
    "def mlp_ff(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['h1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['h2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['h3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    # out_layer = tf.nn.softmax(out_layer)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-5fc61e64b658>:49: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "test_sents = list(conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "label_set = set()\n",
    "for sent in train_sents:\n",
    "    for i in range(len(sent)):\n",
    "        label_set.add(sent[i][-1])\n",
    "\n",
    "label_encode = dict([(list(label_set)[index], index) for index in range(len(label_set))])\n",
    "label_decode = dict([(index, list(label_set)[index]) for index in range(len(label_set))])\n",
    "\n",
    "# placeholder\n",
    "n_input = len(word2features(['default'], 0))\n",
    "n_classes = len(label_set)\n",
    "x = tf.placeholder('float', [None, n_input])\n",
    "y = tf.placeholder('float', [None, n_classes])\n",
    "\n",
    "# weights\n",
    "weights = {\n",
    "    'h1': tf.Variable(\n",
    "        tf.random_uniform([n_input, n_hidden_1], minval=- math.sqrt(6) / 40, maxval=math.sqrt(6) / 40),\n",
    "        name='w_h1'),\n",
    "    'h2': tf.Variable(\n",
    "        tf.random_uniform([n_hidden_1, n_hidden_2], minval=- math.sqrt(6) / 40, maxval=math.sqrt(6) / 40),\n",
    "        name='w_h2'),\n",
    "    'h3': tf.Variable(\n",
    "        tf.random_uniform([n_hidden_2, n_hidden_3], minval=- math.sqrt(6) / 40, maxval=math.sqrt(6) / 40),\n",
    "        name='w_h3'),\n",
    "    'out': tf.Variable(\n",
    "        tf.random_uniform([n_hidden_3, n_classes], minval=- math.sqrt(6) / 40, maxval=math.sqrt(6) / 40),\n",
    "        name='w_out')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_hidden_1]), name='b_h1'),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_2]), name='b_h2'),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_3]), name='b_h3'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='b_out')\n",
    "}\n",
    "\n",
    "# tf.add_to_collection('vars', weights)\n",
    "# tf.add_to_collection('vars', biases)\n",
    "#\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "pred = mlp_ff(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# prediction\n",
    "y_p = tf.argmax(pred, 1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "save_path = ''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: 1000\n",
      "process: 2000\n",
      "process: 3000\n",
      "process: 4000\n",
      "process: 5000\n",
      "process: 6000\n",
      "process: 7000\n",
      "process: 8000\n",
      "Epoch: 0001 cost= 10.794261366\n",
      "./models/tf_mlp.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost = 0\n",
    "        # random.shuffle(batch_list)\n",
    "        count = 0\n",
    "        for sent in train_sents:\n",
    "            count += 1\n",
    "            avg_cost = 0\n",
    "            for i in range(len(sent)):\n",
    "                feats = word2features(sent,i)\n",
    "                train_x = feats\n",
    "                train_y = [x == label_encode[sent[i][-1]] for x in range(n_classes)]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: [train_x], y: [train_y]})\n",
    "                avg_cost += c\n",
    "            if count % 1000 == 0:\n",
    "                print('process:', count)\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \\\n",
    "              \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    save_path = saver.save(sess, './models/tf_mlp.ckpt')\n",
    "    print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/tf_mlp.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./models/tf_mlp.ckpt\n",
      "Model restored from file: ./models/tf_mlp.ckpt\n",
      "Now run: python conlleval.py unconstrained_results.txt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    print(save_path)\n",
    "    saver.restore(sess, './models/tf_mlp.ckpt')\n",
    "    print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "    with open(\"unconstrained_results.txt\", \"w\") as out:\n",
    "        for sent in dev_sents:\n",
    "            for i in range(len(sent)):\n",
    "                feats = word2features(sent,i)\n",
    "                test_x = feats\n",
    "                test_y = [x == label_encode[sent[i][-1]] for x in range(n_classes)]\n",
    "\n",
    "                y_pred = sess.run(pred, feed_dict={x: [test_x], y: [test_y]})\n",
    "                y_pred = label_decode[np.argmax(y_pred)]\n",
    "                word = sent[i][0]\n",
    "                gold = sent[i][-1]\n",
    "                out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,y_pred))\n",
    "        out.write(\"\\n\")\n",
    "\n",
    "print(\"Now run: python conlleval.py unconstrained_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 52923 tokens with 4351 phrases; found: 4979 phrases; correct: 2626.\n",
      "accuracy:  93.69%; precision:  52.74%; recall:  60.35%; FB1:  56.29\n",
      "              LOC: precision:  52.46%; recall:  75.71%; FB1:  61.98  1420\n",
      "             MISC: precision:  27.61%; recall:  42.25%; FB1:  33.39  681\n",
      "              ORG: precision:  57.52%; recall:  54.41%; FB1:  55.93  1608\n",
      "              PER: precision:  60.47%; recall:  62.85%; FB1:  61.64  1270\n"
     ]
    }
   ],
   "source": [
    "import conlleval\n",
    "\n",
    "conlleval.main([\"\", \"unconstrained_results.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
