{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## ASSIGNMENT 1 CODE SKELETON\n",
    "## RELEASED: 2/6/2019\n",
    "## DUE: 2/15/2019\n",
    "## DESCRIPTION: In this assignment, you will explore the\n",
    "## text classification problem of identifying complex words.\n",
    "## We have provided the following skeleton for your code,\n",
    "## with several helper functions, and all the required\n",
    "## functions you need to write.\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "training_file = \"data/complex_words_training.txt\"\n",
    "development_file = \"data/complex_words_development.txt\"\n",
    "test_file = \"data/complex_words_test_unlabeled.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true):\n",
    "    ## YOUR CODE HERE...\n",
    "    TP, FP = 0, 0\n",
    "    \n",
    "    for index in range(len(y_pred)):\n",
    "        if y_pred[index] == 1:\n",
    "            if y_true[index] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "\n",
    "    return precision\n",
    "    \n",
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    ## YOUR CODE HERE...\n",
    "    TP, FN = 0, 0\n",
    "\n",
    "    for index in range(len(y_pred)):\n",
    "        if y_pred[index] == y_true[index] == 1:\n",
    "            TP += 1\n",
    "        elif y_pred[index] == 0 != y_true[index]:\n",
    "            FN += 1\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    return recall\n",
    "\n",
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    ## YOUR CODE HERE...\n",
    "    P = get_precision(y_pred, y_true)\n",
    "    R = get_recall(y_pred, y_true)\n",
    "\n",
    "    fscore = 2 * P * R / (P + R)\n",
    "\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complex Word Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loads in the words and labels of one of the datasets\n",
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: A very simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------2.1 Test---------------------------------------\n",
      "training performance: {'precision': 0.43133333333333335, 'recall': 1.0, 'fscore': 0.6027014438751747}\n",
      "development performance: {'precision': 0.418, 'recall': 1.0, 'fscore': 0.5895627644569816}\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Makes feature matrix for all complex\n",
    "def all_complex_feature(words):\n",
    "    return [1 for x in range(len(words))]\n",
    "\n",
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    ## YOUR CODE HERE...\n",
    "    words, y_true = load_file(data_file)\n",
    "\n",
    "    y_pred = all_complex_feature(words)\n",
    "\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    fscore = get_fscore(y_pred, y_true)\n",
    "    \n",
    "    performance = {'precision': precision, 'recall': recall, 'fscore': fscore}\n",
    "    return performance\n",
    "\n",
    "# ------------------2.1 Test---------------------------------------\n",
    "print('------------------2.1 Test---------------------------------------')\n",
    "\n",
    "performance_training = all_complex(training_file)\n",
    "print('training performance:', performance_training)\n",
    "\n",
    "performance_development = all_complex(development_file)\n",
    "print('development performance:', performance_development)\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Word length thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------2.2 Test---------------------------------------\n",
      "the best threshould for training data (using F1-score): 7\n",
      "training performance: {'precision': 0.5985877240630092, 'recall': 0.8516228748068007, 'fscore': 0.7030303030303029}\n",
      "development performance: {'precision': 0.6053511705685619, 'recall': 0.8660287081339713, 'fscore': 0.7125984251968505}\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Makes feature matrix for word_length_threshold\n",
    "def length_threshold_feature(words, threshold):\n",
    "    y_pred = []\n",
    "    for word in words:\n",
    "        if len(word) < threshold:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "    return y_pred\n",
    "\n",
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    ## YOUR CODE HERE\n",
    "    training_words, training_y_true = load_file(training_file)\n",
    "    development_words, development_y_true = load_file(development_file)\n",
    "\n",
    "    best_threshold = 0\n",
    "    max_fscore = 0\n",
    "    best_training_performance = None\n",
    "    for threshold in range(1, 20):\n",
    "        # training_performance = [tprecision, trecall, tfscore]\n",
    "        training_y_pred = length_threshold_feature(training_words, threshold)\n",
    "            \n",
    "        training_performance = {'precision': get_precision(training_y_pred, training_y_true),\n",
    "                                'recall': get_recall(training_y_pred, training_y_true),\n",
    "                                'fscore': get_fscore(training_y_pred, training_y_true)}\n",
    "        if max_fscore < training_performance['fscore']:\n",
    "            max_fscore = training_performance['fscore']\n",
    "            best_training_performance = training_performance\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print('the best threshould for training data (using F1-score):', best_threshold)\n",
    "    \n",
    "    # development_performance = [dprecision, drecall, dfscore]\n",
    "    development_y_pred = length_threshold_feature(development_words, best_threshold)\n",
    "    development_performance = {'precision': get_precision(development_y_pred, development_y_true),\n",
    "                            'recall': get_recall(development_y_pred, development_y_true),\n",
    "                            'fscore': get_fscore(development_y_pred, development_y_true)}\n",
    "    \n",
    "    return best_training_performance, development_performance\n",
    "\n",
    "# ------------------2.2 Test---------------------------------------\n",
    "print('------------------2.2 Test---------------------------------------')\n",
    "\n",
    "performance_training, performance_development = word_length_threshold(training_file, development_file)\n",
    "print('training performance:', performance_training)\n",
    "print('development performance:', performance_development)\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Word frequency thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loads Google NGram counts\n",
    "def load_ngram_counts(ngram_counts_file): \n",
    "   counts = defaultdict(int) \n",
    "   with gzip.open(ngram_counts_file, 'rt', encoding='UTF-8') as f: \n",
    "       for line in f:\n",
    "           token, count = line.strip().split('\\t') \n",
    "           if token[0].islower(): \n",
    "               counts[token] = int(count) \n",
    "   return counts\n",
    "\n",
    "ngram_counts_file = \"ngram_counts.txt.gz\"\n",
    "counts = load_ngram_counts(ngram_counts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------2.3 Test---------------------------------------\n",
      "max_fscore_approx = 0.602841835546238 in threshold scope ( 41 - 47376829650 )\n",
      "max_fscore_approx = 0.602841835546238 in threshold scope ( 41 - 47376829650 )\n",
      "max_fscore_approx = 0.6358897989575577 in threshold scope ( 41 - 200000041 )\n",
      "max_fscore_approx = 0.6685516808042729 in threshold scope ( 41 - 30000041 )\n",
      "max_fscore_approx = 0.6685516808042729 in threshold scope ( 19000041 - 21000041 )\n",
      "max_fscore_approx = 0.6693954659949621 in threshold scope ( 19600041 - 20000041 )\n",
      "max_fscore_approx = 0.6693980460132367 in threshold scope ( 19720041 - 19740041 )\n",
      "max_fscore_approx = 0.6693980460132367 in threshold scope ( 19727041 - 19732041 )\n",
      "max_fscore_approx = 0.6693980460132367 in threshold scope ( 19727741 - 19732041 )\n",
      "max_fscore_approx = 0.6693980460132367 in threshold scope ( 19727781 - 19732011 )\n",
      "max_fscore_approx = 0.6693980460132367 in threshold scope ( 19727786 - 19732011 )\n",
      "---------------------------\n",
      "best threshold for training data: 19732010 , best fscore: 0.6693980460132367\n",
      "---------------------------\n",
      "max_fscore_approx = 0.5895627644569816 in threshold scope ( 1 - 47376829650 )\n",
      "max_fscore_approx = 0.5899788285109385 in threshold scope ( 1 - 2000000001 )\n",
      "max_fscore_approx = 0.6175807663410969 in threshold scope ( 1 - 200000001 )\n",
      "max_fscore_approx = 0.6704653371320038 in threshold scope ( 1 - 30000001 )\n",
      "max_fscore_approx = 0.6759720837487537 in threshold scope ( 13000001 - 16000001 )\n",
      "max_fscore_approx = 0.6779999999999999 in threshold scope ( 14100001 - 15000001 )\n",
      "max_fscore_approx = 0.6779999999999999 in threshold scope ( 14870001 - 14930001 )\n",
      "max_fscore_approx = 0.6779999999999999 in threshold scope ( 14873001 - 14921001 )\n",
      "max_fscore_approx = 0.6779999999999999 in threshold scope ( 14873701 - 14920201 )\n",
      "max_fscore_approx = 0.6779999999999999 in threshold scope ( 14873731 - 14920111 )\n",
      "max_fscore_approx = 0.6779999999999999 in threshold scope ( 14873738 - 14920111 )\n",
      "---------------------------\n",
      "best threshold for development data: 14920110 , best fscore: 0.6779999999999999\n",
      "---------------------------\n",
      "training performance: {'precision': 0.5651942522618414, 'recall': 0.8207109737248841, 'fscore': 0.6693980460132367}\n",
      "development performance: {'precision': 0.5824742268041238, 'recall': 0.8110047846889952, 'fscore': 0.6779999999999999}\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "\n",
    "## Make feature matrix for word_frequency_threshold\n",
    "def frequency_threshold_feature(words, threshold, counts):\n",
    "    y_pred = []\n",
    "    for word in words:\n",
    "        if counts[word] < threshold:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    return y_pred\n",
    "\n",
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "    ## YOUR CODE HERE\n",
    "    training_words, training_y_true = load_file(training_file)\n",
    "    development_words, development_y_true = load_file(development_file)\n",
    "    \n",
    "    def find_best_threshold(words, y_true):\n",
    "        min_freq = np.min(list(counts.values()))\n",
    "        max_freq = np.max(list(counts.values()))\n",
    "        \n",
    "        max_fscore = 0\n",
    "        best_threshold = 0\n",
    "        \n",
    "        scope = 10000000000\n",
    "        lower, upper = min_freq + 1, max_freq\n",
    "        while(scope > 0):\n",
    "            current_lower, current_upper = lower, upper\n",
    "            flag = False\n",
    "            for threshold in range(current_lower, current_upper, scope):\n",
    "                y_pred = frequency_threshold_feature(words, threshold, counts)\n",
    "                fscore = get_fscore(y_pred, y_true)\n",
    "                if fscore >= max_fscore:\n",
    "                    max_fscore = fscore\n",
    "                    best_threshold = threshold\n",
    "                    if flag == False:\n",
    "                        lower = max(min_freq + 1, threshold - scope)\n",
    "                        flag = True\n",
    "                    upper = min(threshold + scope, max_freq - 1)\n",
    "            \n",
    "            scope = int(scope / 10)\n",
    "            \n",
    "            print('max_fscore_approx =', max_fscore, 'in threshold scope (', lower, '-', upper, ')')\n",
    "            \n",
    "        return best_threshold, fscore\n",
    "    \n",
    "    training_best_threshold, training_fscore = find_best_threshold(training_words, training_y_true)\n",
    "    \n",
    "    print('---------------------------')\n",
    "    print(\"best threshold for training data:\", training_best_threshold, \", best fscore:\", training_fscore)\n",
    "    print('---------------------------')\n",
    "    \n",
    "    training_y_pred = frequency_threshold_feature(training_words, training_best_threshold, counts)\n",
    "    # training_performance = [tprecision, trecall, tfscore]\n",
    "    training_performance = {'precision': get_precision(training_y_pred, training_y_true),\n",
    "                                'recall': get_recall(training_y_pred, training_y_true),\n",
    "                                'fscore': get_fscore(training_y_pred, training_y_true)}\n",
    "    \n",
    "    development_best_threshold, development_fscore = find_best_threshold(development_words, development_y_true)\n",
    "    \n",
    "    print('---------------------------')\n",
    "    print(\"best threshold for development data:\", development_best_threshold, \", best fscore:\", development_fscore)\n",
    "    print('---------------------------')\n",
    "    \n",
    "    development_y_pred = frequency_threshold_feature(development_words, development_best_threshold, counts)\n",
    "    # development_performance = [dprecision, drecall, dfscore]\n",
    "    development_performance = {'precision': get_precision(development_y_pred, development_y_true),\n",
    "                                'recall': get_recall(development_y_pred, development_y_true),\n",
    "                                'fscore': get_fscore(development_y_pred, development_y_true)}\n",
    "    \n",
    "    return training_performance, development_performance\n",
    "\n",
    "\n",
    "# ------------------2.3 Test---------------------------------------\n",
    "print('------------------2.3 Test---------------------------------------')\n",
    "\n",
    "training_performance, development_performance = word_frequency_threshold(training_file, development_file, counts)\n",
    "print('training performance:', training_performance)\n",
    "print('development performance:', development_performance)\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_length_and_frequency(words, counts, mean=None, std=None):\n",
    "        length_frequency = []\n",
    "        for word in words:\n",
    "            length_frequency.append([len(word), counts[word]])\n",
    "        \n",
    "        length_frequency = np.array(length_frequency)\n",
    "        \n",
    "        if mean is None:\n",
    "            mean = np.mean(length_frequency, axis=0)\n",
    "            std = np.std(length_frequency, axis=0)\n",
    "        normalized_length_frequency = (length_frequency - mean) / std\n",
    "        \n",
    "        return normalized_length_frequency, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------2.4 Test---------------------------------------\n",
      "training performance: {'precision': 0.4918351477449456, 'recall': 0.9775888717156105, 'fscore': 0.6544231764097258}\n",
      "development performance: {'precision': 0.4700352526439483, 'recall': 0.9569377990430622, 'fscore': 0.6304176516942475}\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def naive_bayes(training_file, development_file, counts):\n",
    "    ## YOUR CODE HERE\n",
    "    training_words, training_y_true = load_file(training_file)\n",
    "    development_words, development_y_true = load_file(development_file)\n",
    "    \n",
    "    X_train, training_mean, training_std = get_length_and_frequency(training_words, counts)\n",
    "    Y_train = np.array(training_y_true)\n",
    "    \n",
    "    NB_classifier = GaussianNB()\n",
    "    NB_classifier.fit(X_train, Y_train)\n",
    "    \n",
    "    X_development, _, _ = get_length_and_frequency(development_words, counts, training_mean, training_std)\n",
    "    Y_development = np.array(development_y_true)\n",
    "    development_y_pred = NB_classifier.predict(X_development)\n",
    "    \n",
    "    # development_performance = (dprecision, drecall, dfscore)\n",
    "    development_performance = {'precision': get_precision(development_y_pred, development_y_true),\n",
    "                                'recall': get_recall(development_y_pred, development_y_true),\n",
    "                                'fscore': get_fscore(development_y_pred, development_y_true)}\n",
    "    return development_performance\n",
    "\n",
    "# ------------------2.4 Test---------------------------------------\n",
    "print('------------------2.4 Test---------------------------------------')\n",
    "\n",
    "training_performance = naive_bayes(training_file, training_file, counts)\n",
    "development_performance = naive_bayes(training_file, development_file, counts)\n",
    "print('training performance:', training_performance)\n",
    "print('development performance:', development_performance)\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------2.5 Test---------------------------------------\n",
      "training performance: {'precision': 0.7206751054852321, 'recall': 0.6599690880989181, 'fscore': 0.6889874949576441}\n",
      "development performance: {'precision': 0.7229219143576826, 'recall': 0.6866028708133971, 'fscore': 0.7042944785276073}\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression(training_file, development_file, counts):\n",
    "    ## YOUR CODE HERE\n",
    "    training_words, training_y_true = load_file(training_file)\n",
    "    development_words, development_y_true = load_file(development_file)\n",
    "    \n",
    "    X_train, training_mean, training_std = get_length_and_frequency(training_words, counts)\n",
    "    Y_train = np.array(training_y_true)\n",
    "    \n",
    "    LR_classifier = LogisticRegression()\n",
    "    LR_classifier.fit(X_train, Y_train)\n",
    "    \n",
    "    X_development, _, _ = get_length_and_frequency(development_words, counts, training_mean, training_std)\n",
    "    Y_development = np.array(development_y_true)\n",
    "    development_y_pred = LR_classifier.predict(X_development)\n",
    "    \n",
    "    # development_performance = (dprecision, drecall, dfscore)\n",
    "    development_performance = {'precision': get_precision(development_y_pred, development_y_true),\n",
    "                                'recall': get_recall(development_y_pred, development_y_true),\n",
    "                                'fscore': get_fscore(development_y_pred, development_y_true)}\n",
    "    return development_performance\n",
    "\n",
    "# ------------------2.5 Test---------------------------------------\n",
    "print('------------------2.5 Test---------------------------------------')\n",
    "\n",
    "training_performance = logistic_regression(training_file, training_file, counts)\n",
    "development_performance = logistic_regression(training_file, development_file, counts)\n",
    "print('training performance:', training_performance)\n",
    "print('development performance:', development_performance)\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7: Build your own classifier\n",
    "\n",
    "### features used:\n",
    "1. word length\n",
    "2. word frequency\n",
    "3. syllable number\n",
    "4. synonym number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import syllables\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "# generate feature data\n",
    "def get_features(words, counts, normalize_mean=None, normalize_std=None):\n",
    "    length_frequency, normalize_mean, normalize_std = get_length_and_frequency(words, counts, normalize_mean, normalize_std)\n",
    "    syllable_count = []\n",
    "    synonym_count= []\n",
    "    frequency_ratio = []\n",
    "    \n",
    "    for word in words:\n",
    "        syllable_count.append([syllables.count_syllables(word)])\n",
    "        \n",
    "        synonym = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for orginal in syn.lemmas():\n",
    "                synonym.append(orginal.name())\n",
    "        synonym = set(synonym)\n",
    "        \n",
    "        synonym_count.append([len(synonym)])\n",
    "        \n",
    "    features = np.concatenate((np.array(length_frequency), np.array(syllable_count), np.array(synonym_count)), axis=1)\n",
    "    \n",
    "    return features, normalize_mean, normalize_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the performance (precision, recall, fscore)\n",
    "def get_performance(y_pred, y_true):\n",
    "    performance = {'precision': get_precision(y_pred, y_true),\n",
    "                'recall': get_recall(y_pred, y_true),\n",
    "                'fscore': get_fscore(y_pred, y_true)}\n",
    "    return performance\n",
    "\n",
    "training_file = \"data/complex_words_training.txt\"\n",
    "development_file = \"data/complex_words_development.txt\"\n",
    "test_file = \"data/complex_words_test_unlabeled.txt\"\n",
    "\n",
    "training_words, training_labels = load_file(training_file)\n",
    "development_words, development_labels = load_file(development_file)\n",
    "\n",
    "training_words, training_labels = np.array(training_words), np.array(training_labels)\n",
    "development_words, development_labels = np.array(development_words), np.array(development_labels)\n",
    "\n",
    "training_development_words = np.concatenate((training_words, development_words), axis=0)\n",
    "training_development_labels = np.concatenate((training_labels, development_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perofrmance: {'precision': 0.525096525096525, 'recall': 0.9532710280373832, 'fscore': 0.6771784232365144}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def NB_2(training_words, training_labels, predict_words, counts):\n",
    "    training_features, mean, std = get_features(training_words, counts)\n",
    "    predict_features, _, _ = get_features(predict_words, counts, mean, std)\n",
    "    \n",
    "    NB_classifier = GaussianNB()\n",
    "    NB_classifier.fit(training_features, training_labels)\n",
    "    \n",
    "    y_pred = NB_classifier.predict(predict_features)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "training_development_y_pred = NB_2(training_development_words, training_development_labels, training_development_words, counts)\n",
    "print('perofrmance:', get_performance(training_development_y_pred, training_development_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perofrmance: {'precision': 0.7235221674876847, 'recall': 0.6863317757009346, 'fscore': 0.7044364508393286}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVM(training_words, training_labels, predict_words, counts):\n",
    "    training_features, mean, std = get_features(training_words, counts)\n",
    "    predict_features, _, _ = get_features(predict_words, counts, mean, std)\n",
    "    \n",
    "    SVM_classifier = SVC()\n",
    "    SVM_classifier.fit(training_features, training_labels)\n",
    "    \n",
    "    y_pred = SVM_classifier.predict(predict_features)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "training_development_y_pred = SVM(training_development_words, training_development_labels, training_development_words, counts)\n",
    "print('perofrmance:', get_performance(training_development_y_pred, training_development_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 3: K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perofrmance: {'precision': 0.8227997637330183, 'recall': 0.8136682242990654, 'fscore': 0.8182085168869309}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def KNN(training_words, training_labels, predict_words, counts):\n",
    "    training_features, mean, std = get_features(training_words, counts)\n",
    "    predict_features, _, _ = get_features(predict_words, counts, mean, std)\n",
    "    \n",
    "    KNN_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "    KNN_classifier.fit(training_features, training_labels)\n",
    "        \n",
    "    y_pred = KNN_classifier.predict(predict_features)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "training_development_y_pred = KNN(training_development_words, training_development_labels, training_development_words, counts)\n",
    "print('perofrmance:', get_performance(training_development_y_pred, training_development_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 4: Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perofrmance: {'precision': 0.7217194570135747, 'recall': 0.7453271028037384, 'fscore': 0.7333333333333333}\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers.core import Activation, Dense\n",
    "import h5py\n",
    "\n",
    "def build_model(training_words, training_labels):\n",
    "    training_features, mean, std = get_features(training_words, counts)\n",
    "    NN_model = keras.models.Sequential()\n",
    "    adam_opt = keras.optimizers.Adam(lr=0.001)\n",
    "    \n",
    "    NN_model.add(Dense(32, input_shape=(4,)))\n",
    "    NN_model.add(Activation('relu'))\n",
    "    NN_model.add(Dense(32))\n",
    "    NN_model.add(Activation('relu'))\n",
    "    NN_model.add(Dense(1))\n",
    "    NN_model.add(Activation('sigmoid'))\n",
    "    \n",
    "    NN_model.compile(optimizer=adam_opt, loss='mse')\n",
    "    NN_model.fit(training_features, training_labels, epochs=100)\n",
    "    \n",
    "    NN_model.save('neural_network.h5')\n",
    "\n",
    "# build_model(training_development_words, training_development_labels)\n",
    "def neural_network(NN_model, training_words, training_labels, predict_words):\n",
    "    training_features, mean, std = get_features(training_words, counts)\n",
    "    predict_features, _, _ = get_features(predict_words, counts, mean, std)\n",
    "    \n",
    "    y_pred = NN_model.predict(predict_features)\n",
    "    \n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "NN_model = keras.models.load_model('neural_network.h5')\n",
    "training_development_y_pred = neural_network(NN_model, training_development_words, training_development_labels, training_development_words)\n",
    "print('perofrmance:', get_performance(training_development_y_pred, training_development_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combo model: decision maker (with neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2258\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 97us/step - loss: 0.2296\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.1911\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.1842\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 107us/step - loss: 0.1772\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 99us/step - loss: 0.1978\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.1735\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.1788\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.1669\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.1751\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.1692\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 92us/step - loss: 0.1651\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 97us/step - loss: 0.1635\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.1628\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.1678\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.1609\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 100us/step - loss: 0.1694\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 107us/step - loss: 0.1852\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 91us/step - loss: 0.1756\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.1737\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.1764\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.1754\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.1811\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 97us/step - loss: 0.1598\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 87us/step - loss: 0.1655\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.1782\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.1608\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.1643\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.1708\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 97us/step - loss: 0.1865\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.1757\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 104us/step - loss: 0.1657\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 162us/step - loss: 0.1583\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.1848\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.1669\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 107us/step - loss: 0.1757\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.1568\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 91us/step - loss: 0.1764\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.1734\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.1695\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.1825\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 97us/step - loss: 0.1598\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 92us/step - loss: 0.1685\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 90us/step - loss: 0.1843\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.1693\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.1625\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 95us/step - loss: 0.1769\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 107us/step - loss: 0.1723\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.1717\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.1619\n"
     ]
    }
   ],
   "source": [
    "def build_coef_net(training_words, training_labels, counts):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    NN_model = keras.models.load_model('neural_network.h5')\n",
    "    \n",
    "    classifier_model = keras.models.Sequential()\n",
    "    adam_opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "    classifier_model.add(Dense(32, input_shape=(4,)))\n",
    "    classifier_model.add(Activation('relu'))\n",
    "    classifier_model.add(Dense(32))\n",
    "    classifier_model.add(Activation('relu'))\n",
    "    classifier_model.add(Dense(1))\n",
    "    classifier_model.add(Activation('sigmoid'))\n",
    "\n",
    "    classifier_model.compile(optimizer=adam_opt, loss='mse')\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(training_words, training_labels, test_size=0.2)\n",
    "        \n",
    "        y_pred_NB = NB_2(X_train, y_train, X_test, counts).reshape(-1, 1)\n",
    "        y_pred_SVM = SVM(X_train, y_train, X_test, counts).reshape(-1, 1)\n",
    "        y_pred_KNN = KNN(X_train, y_train, X_test, counts).reshape(-1, 1)\n",
    "        y_pred_NN = neural_network(NN_model, X_train, y_train, X_test).reshape(-1, 1)\n",
    "\n",
    "        predict_features = np.concatenate((y_pred_NB, y_pred_SVM, y_pred_KNN, y_pred_NN), axis=1)\n",
    "        classifier_model.fit(predict_features, y_test)\n",
    "    \n",
    "    classifier_model.save('main_classifier_model.h5')\n",
    "\n",
    "# build_coef_net(training_development_words, training_development_labels, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My complex word classifier\n",
    "1. use multiple models to predict the labels\n",
    "2. use decision maker model to judge and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perofrmance: {'precision': 0.7533414337788579, 'recall': 0.7242990654205608, 'fscore': 0.7385348421679571}\n"
     ]
    }
   ],
   "source": [
    "def decision_model(y_pred_NB, y_pred_SVM, y_pred_KNN, y_pred_NN):\n",
    "    predict_features = np.concatenate((y_pred_NB, y_pred_SVM, y_pred_KNN, y_pred_NN), axis=1)\n",
    "    \n",
    "    classifier_model = keras.models.load_model('main_classifier_model.h5')\n",
    "    y_pred = classifier_model.predict(predict_features)\n",
    "    \n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def main_classifier(training_words, training_labels, predict_words, counts):\n",
    "    training_features, mean, std = get_features(training_words, counts)\n",
    "    predict_features, _, _ = get_features(predict_words, counts, mean, std)\n",
    "    \n",
    "    y_pred_NB = NB_2(training_words, training_labels, predict_words, counts).reshape(-1, 1)\n",
    "    y_pred_SVM = SVM(training_words, training_labels, predict_words, counts).reshape(-1, 1)\n",
    "    y_pred_KNN = KNN(training_words, training_labels, predict_words, counts).reshape(-1, 1)\n",
    "    y_pred_NN = neural_network(NN_model, training_words, training_labels, predict_words).reshape(-1, 1)\n",
    "    \n",
    "    y_pred = decision_model(y_pred_NB, y_pred_SVM, y_pred_KNN, y_pred_NN)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "training_development_y_pred = main_classifier(training_development_words, training_development_labels, training_development_words, counts)\n",
    "print('perofrmance:', get_performance(training_development_y_pred, training_development_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: (1000, 1)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_file = \"data/complex_words_test_unlabeled.txt\"\n",
    "    test_words = []\n",
    "    with open(test_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                test_words.append(line_split[0].lower())\n",
    "            i += 1\n",
    "    \n",
    "    test_words = np.array(test_words)\n",
    "    \n",
    "    test_y_pred = main_classifier(training_development_words, training_development_labels, test_words, counts)\n",
    "    print('output size:', test_y_pred.shape)\n",
    "    \n",
    "    with open('test_labels.txt', 'w') as f:\n",
    "        for label in test_y_pred:\n",
    "            f.writelines(str(int(label[0])) + '\\n')\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
