{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_shakespeare():\n",
    "    '''Reads in the Shakespeare dataset processesit into a list of tuples.\n",
    "        Also reads in the vocab and play name lists from files.\n",
    "\n",
    "        Each tuple consists of\n",
    "        tuple[0]: The name of the play\n",
    "        tuple[1] A line from the play as a list of tokenized words.\n",
    "\n",
    "        Returns:\n",
    "        tuples: A list of tuples in the above format.\n",
    "        document_names: A list of the plays present in the corpus.\n",
    "        vocab: A list of all tokens in the vocabulary.\n",
    "    '''\n",
    "\n",
    "    tuples = []\n",
    "    \n",
    "    with open('will_play_text.csv') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=';')\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "\n",
    "            tuples.append((play_name, line_tokens))\n",
    "\n",
    "    with open('vocab.txt') as f:\n",
    "        vocab =  [line.strip() for line in f]\n",
    "\n",
    "    with open('play_names.txt') as f:\n",
    "        document_names =  [line.strip() for line in f]\n",
    "\n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "tuples, document_names, vocab = read_in_shakespeare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_term_document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_vector(matrix, row_id):\n",
    "    return matrix[row_id, :]\n",
    "\n",
    "def get_column_vector(matrix, col_id):\n",
    "    return matrix[:, col_id]\n",
    "\n",
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    '''Returns a numpy array containing the term document matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    document_names: A list of the document names\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "\n",
    "    Returns:\n",
    "    td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "        and each column corresponds to a document. A_ij contains the\n",
    "        frequency with which word i occurs in document j.\n",
    "    '''\n",
    "\n",
    "    vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "    docname_to_id = dict(zip(document_names, range(0, len(document_names))))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    matrix = np.zeros((len(vocab), len(document_names)))\n",
    "    \n",
    "    for line in line_tuples:\n",
    "        doc_name = line[0]\n",
    "        words = line[1]\n",
    "        for word in words:\n",
    "            matrix[vocab_to_id[word]][docname_to_id[doc_name]] += 1\n",
    "            \n",
    "    return matrix\n",
    "\n",
    "\n",
    "term_document_matrix = create_term_document_matrix(tuples, document_names, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the closest play to Henry IV ---is--- The Tempest\n",
      "the closest play to Alls well that ends well ---is--- A Winters Tale\n",
      "the closest play to Loves Labours Lost ---is--- Merchant of Venice\n",
      "the closest play to Taming of the Shrew ---is--- Much Ado about nothing\n",
      "the closest play to Antony and Cleopatra ---is--- Cymbeline\n",
      "the closest play to Coriolanus ---is--- Cymbeline\n",
      "the closest play to Hamlet ---is--- Henry VIII\n",
      "the closest play to A Midsummer nights dream ---is--- As you like it\n",
      "the closest play to Merry Wives of Windsor ---is--- Twelfth Night\n",
      "the closest play to Romeo and Juliet ---is--- A Midsummer nights dream\n",
      "the closest play to Richard II ---is--- Henry VI Part 2\n",
      "the closest play to King John ---is--- Henry VI Part 1\n",
      "the closest play to macbeth ---is--- Hamlet\n",
      "the closest play to Timon of Athens ---is--- King Lear\n",
      "the closest play to A Winters Tale ---is--- Cymbeline\n",
      "the closest play to The Tempest ---is--- King Lear\n",
      "the closest play to Henry VI Part 2 ---is--- Henry VI Part 1\n",
      "the closest play to As you like it ---is--- Alls well that ends well\n",
      "the closest play to Julius Caesar ---is--- Antony and Cleopatra\n",
      "the closest play to A Comedy of Errors ---is--- As you like it\n",
      "the closest play to Henry VIII ---is--- Hamlet\n",
      "the closest play to Measure for measure ---is--- Alls well that ends well\n",
      "the closest play to Richard III ---is--- Richard II\n",
      "the closest play to Two Gentlemen of Verona ---is--- Othello\n",
      "the closest play to Henry VI Part 1 ---is--- Henry VI Part 2\n",
      "the closest play to Much Ado about nothing ---is--- As you like it\n",
      "the closest play to Henry V ---is--- King John\n",
      "the closest play to Troilus and Cressida ---is--- Hamlet\n",
      "the closest play to Twelfth Night ---is--- As you like it\n",
      "the closest play to Merchant of Venice ---is--- As you like it\n",
      "the closest play to Henry VI Part 3 ---is--- Henry VI Part 2\n",
      "the closest play to Othello ---is--- Alls well that ends well\n",
      "the closest play to Cymbeline ---is--- A Winters Tale\n",
      "the closest play to King Lear ---is--- Cymbeline\n",
      "the closest play to Pericles ---is--- A Winters Tale\n",
      "the closest play to Titus Andronicus ---is--- Henry VI Part 2\n"
     ]
    }
   ],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "    Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "    Returns:\n",
    "    A scalar similarity value.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    inner = np.inner(vector1, vector2)\n",
    "    length = (np.sqrt(np.sum(vector1 ** 2)) * np.sqrt(np.sum(vector2 ** 2)))\n",
    "    \n",
    "    if length == 0:\n",
    "        return 0\n",
    "    \n",
    "    return inner / length\n",
    "\n",
    "\n",
    "similarity_matrix_cos = np.zeros((len(document_names), len(document_names)))\n",
    "\n",
    "for x in range(len(document_names)):\n",
    "    for y in range(x + 1, len(document_names)):\n",
    "        similarity_matrix_cos[x][y] = similarity_matrix_cos[y][x] = compute_cosine_similarity(get_column_vector(term_document_matrix, x), get_column_vector(term_document_matrix, y))\n",
    "\n",
    "for index in range(len(similarity_matrix_cos)):\n",
    "    closest_id = np.argmax(similarity_matrix_cos[index])\n",
    "    print('the closest play to', document_names[index], '---is---', document_names[closest_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_term_context_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "    '''Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    Let n = len(vocab).\n",
    "\n",
    "    Returns:\n",
    "    tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "        word j was found within context_window_size to the left or right of\n",
    "        word i in any sentence in the tuples.\n",
    "    '''\n",
    "\n",
    "    vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    term_context_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    for line in line_tuples:\n",
    "        for word_id in range(len(line[1])):\n",
    "            word = line[1][word_id]\n",
    "            # term_context_matrix[vocab_to_id[word]][vocab_to_id[word]] += 1\n",
    "            for shift in range(1, context_window_size + 1):\n",
    "                left = word_id - shift\n",
    "                right = word_id + shift\n",
    "                if left >= 0:\n",
    "                    term_context_matrix[vocab_to_id[word]][vocab_to_id[line[1][left]]] += 1\n",
    "                if right < len(line[1]):\n",
    "                    term_context_matrix[vocab_to_id[word]][vocab_to_id[line[1][right]]] += 1\n",
    "            \n",
    "    return term_context_matrix\n",
    "\n",
    "term_context_matrix = create_term_context_matrix(tuples, vocab, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_PPMI_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_PPMI_matrix(term_context_matrix):\n",
    "    '''Given a term context matrix, output a PPMI matrix.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "    term_context_matrix: A nxn numpy array, where n is\n",
    "    the numer of tokens in the vocab.\n",
    "\n",
    "    Returns: A nxn numpy matrix, where A_ij is equal to the\n",
    "    point-wise mutual information between the ith word\n",
    "    and the jth word in the term_context_matrix.\n",
    "    '''       \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    from scipy import sparse\n",
    "    \n",
    "    try:\n",
    "        PPMI_matrix = sparse.load_npz(\"PPMI_matrix.npz\").toarray()\n",
    "    except:\n",
    "        PPMI_matrix = np.zeros_like(term_context_matrix)\n",
    "\n",
    "        total = np.sum(term_context_matrix)\n",
    "        count_p_i = np.zeros(term_context_matrix.shape[0])\n",
    "        count_p_j = np.zeros(term_context_matrix.shape[0])\n",
    "        for j in range(term_context_matrix.shape[0]):\n",
    "            count_p_i[j] = np.sum(term_context_matrix[:, j]) / total\n",
    "        for i in range(term_context_matrix.shape[0]):\n",
    "            count_p_j[i] = np.sum(term_context_matrix[i, :]) / total\n",
    "\n",
    "        for i in range(term_context_matrix.shape[0]):\n",
    "            for j in range(term_context_matrix.shape[1]):\n",
    "                p_ij = term_context_matrix[i][j] / total\n",
    "                p_i = count_p_i[j]\n",
    "                p_j = count_p_j[i]\n",
    "\n",
    "                if p_ij > 0:\n",
    "                    PPMI_matrix[i][j] = max(np.log(p_ij / (p_i * p_j)), 0)\n",
    "                else:\n",
    "                    PPMI_matrix[i][j] = 0\n",
    "        \n",
    "        PPMI_matrix = sparse.csc_matrix(PPMI_matrix)\n",
    "        sparse.save_npz(\"PPMI_matrix.npz\",PPMI_matrix)\n",
    "    return PPMI_matrix\n",
    "\n",
    "PPMI_matrix = create_PPMI_matrix(term_context_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 2.19722458, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.17763627, 0.        , ..., 0.69314718, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.28093385, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "    '''Given the term document matrix, output a tf-idf weighted version.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "    term_document_matrix: Numpy array where each column represents a document \n",
    "    and each row, the frequency of a word in that document.\n",
    "\n",
    "    Returns:\n",
    "    A numpy array with the same dimension as term_document_matrix, where\n",
    "    A_ij is weighted by the inverse document frequency of document h.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    num_of_doc = term_document_matrix.shape[1]\n",
    "    tf_idf_matrix = np.zeros_like(term_document_matrix)\n",
    "    for word_id in range(len(term_document_matrix)):\n",
    "        \n",
    "        word_vector = term_document_matrix[word_id]\n",
    "        doc_contain_word = np.sum(word_vector > 0)\n",
    "        for doc_id in range(len(word_vector)):\n",
    "            tf = 0 if word_vector[doc_id] == 0 else 1 + np.log10(word_vector[doc_id])\n",
    "            idf = np.log(num_of_doc / doc_contain_word)\n",
    "            tf_idf_matrix[word_id][doc_id] = tf * idf\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "create_tf_idf_matrix(term_document_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the closest play to Henry IV ---is--- Henry VI Part 2\n",
      "the closest play to Alls well that ends well ---is--- A Winters Tale\n",
      "the closest play to Loves Labours Lost ---is--- Merchant of Venice\n",
      "the closest play to Taming of the Shrew ---is--- Twelfth Night\n",
      "the closest play to Antony and Cleopatra ---is--- Cymbeline\n",
      "the closest play to Coriolanus ---is--- Cymbeline\n",
      "the closest play to Hamlet ---is--- Othello\n",
      "the closest play to A Midsummer nights dream ---is--- The Tempest\n",
      "the closest play to Merry Wives of Windsor ---is--- Much Ado about nothing\n",
      "the closest play to Romeo and Juliet ---is--- Othello\n",
      "the closest play to Richard II ---is--- King John\n",
      "the closest play to King John ---is--- Richard II\n",
      "the closest play to macbeth ---is--- Pericles\n",
      "the closest play to Timon of Athens ---is--- Pericles\n",
      "the closest play to A Winters Tale ---is--- Cymbeline\n",
      "the closest play to The Tempest ---is--- Pericles\n",
      "the closest play to Henry VI Part 2 ---is--- Henry VI Part 3\n",
      "the closest play to As you like it ---is--- Much Ado about nothing\n",
      "the closest play to Julius Caesar ---is--- Antony and Cleopatra\n",
      "the closest play to A Comedy of Errors ---is--- Two Gentlemen of Verona\n",
      "the closest play to Henry VIII ---is--- A Winters Tale\n",
      "the closest play to Measure for measure ---is--- Alls well that ends well\n",
      "the closest play to Richard III ---is--- Henry VI Part 2\n",
      "the closest play to Two Gentlemen of Verona ---is--- Twelfth Night\n",
      "the closest play to Henry VI Part 1 ---is--- Henry VI Part 2\n",
      "the closest play to Much Ado about nothing ---is--- As you like it\n",
      "the closest play to Henry V ---is--- Henry IV\n",
      "the closest play to Troilus and Cressida ---is--- King Lear\n",
      "the closest play to Twelfth Night ---is--- As you like it\n",
      "the closest play to Merchant of Venice ---is--- As you like it\n",
      "the closest play to Henry VI Part 3 ---is--- Henry VI Part 2\n",
      "the closest play to Othello ---is--- Alls well that ends well\n",
      "the closest play to Cymbeline ---is--- A Winters Tale\n",
      "the closest play to King Lear ---is--- Cymbeline\n",
      "the closest play to Pericles ---is--- macbeth\n",
      "the closest play to Titus Andronicus ---is--- Richard II\n"
     ]
    }
   ],
   "source": [
    "def compute_jaccard_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "    Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "    Returns:\n",
    "    A scalar similarity value.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    concatenated_vector = np.concatenate((vector1.reshape(-1, 1), vector2.reshape(-1, 1)), axis=1)\n",
    "    return np.sum(np.min(concatenated_vector, axis=1)) / np.sum(np.max(concatenated_vector, axis=1))\n",
    "\n",
    "similarity_matrix_jaccard = np.zeros((len(document_names), len(document_names)))\n",
    "\n",
    "for x in range(len(document_names)):\n",
    "    for y in range(x + 1, len(document_names)):\n",
    "        similarity_matrix_jaccard[x][y] = similarity_matrix_jaccard[y][x] = compute_jaccard_similarity(get_column_vector(term_document_matrix, x), get_column_vector(term_document_matrix, y))\n",
    "\n",
    "for index in range(len(similarity_matrix_jaccard)):\n",
    "    closest_id = np.argmax(similarity_matrix_jaccard[index])\n",
    "    print('the closest play to', document_names[index], '---is---', document_names[closest_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_dice_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the closest play to Henry IV ---is--- Henry VI Part 2\n",
      "the closest play to Alls well that ends well ---is--- A Winters Tale\n",
      "the closest play to Loves Labours Lost ---is--- Merchant of Venice\n",
      "the closest play to Taming of the Shrew ---is--- Twelfth Night\n",
      "the closest play to Antony and Cleopatra ---is--- Cymbeline\n",
      "the closest play to Coriolanus ---is--- Cymbeline\n",
      "the closest play to Hamlet ---is--- Othello\n",
      "the closest play to A Midsummer nights dream ---is--- The Tempest\n",
      "the closest play to Merry Wives of Windsor ---is--- Much Ado about nothing\n",
      "the closest play to Romeo and Juliet ---is--- Othello\n",
      "the closest play to Richard II ---is--- King John\n",
      "the closest play to King John ---is--- Richard II\n",
      "the closest play to macbeth ---is--- Pericles\n",
      "the closest play to Timon of Athens ---is--- Pericles\n",
      "the closest play to A Winters Tale ---is--- Cymbeline\n",
      "the closest play to The Tempest ---is--- Pericles\n",
      "the closest play to Henry VI Part 2 ---is--- Henry VI Part 3\n",
      "the closest play to As you like it ---is--- Much Ado about nothing\n",
      "the closest play to Julius Caesar ---is--- Antony and Cleopatra\n",
      "the closest play to A Comedy of Errors ---is--- Two Gentlemen of Verona\n",
      "the closest play to Henry VIII ---is--- A Winters Tale\n",
      "the closest play to Measure for measure ---is--- Alls well that ends well\n",
      "the closest play to Richard III ---is--- Henry VI Part 2\n",
      "the closest play to Two Gentlemen of Verona ---is--- Twelfth Night\n",
      "the closest play to Henry VI Part 1 ---is--- Henry VI Part 2\n",
      "the closest play to Much Ado about nothing ---is--- As you like it\n",
      "the closest play to Henry V ---is--- Henry IV\n",
      "the closest play to Troilus and Cressida ---is--- King Lear\n",
      "the closest play to Twelfth Night ---is--- As you like it\n",
      "the closest play to Merchant of Venice ---is--- As you like it\n",
      "the closest play to Henry VI Part 3 ---is--- Henry VI Part 2\n",
      "the closest play to Othello ---is--- Alls well that ends well\n",
      "the closest play to Cymbeline ---is--- A Winters Tale\n",
      "the closest play to King Lear ---is--- Cymbeline\n",
      "the closest play to Pericles ---is--- macbeth\n",
      "the closest play to Titus Andronicus ---is--- Richard II\n"
     ]
    }
   ],
   "source": [
    "def compute_dice_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "    Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "    Returns:\n",
    "    A scalar similarity value.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE  2 * sum(min(a,b)) / sum(a+b)\n",
    "    concatenated_vector = np.concatenate((vector1.reshape(-1, 1), vector2.reshape(-1, 1)), axis=1)\n",
    "    return 2 * np.sum(np.min(concatenated_vector, axis=1)) / np.sum(vector1 + vector2)\n",
    "\n",
    "similarity_matrix_dice = np.zeros((len(document_names), len(document_names)))\n",
    "\n",
    "for x in range(len(document_names)):\n",
    "    for y in range(x + 1, len(document_names)):\n",
    "        similarity_matrix_dice[x][y] = similarity_matrix_dice[y][x] = compute_dice_similarity(get_column_vector(term_document_matrix, x), get_column_vector(term_document_matrix, y))\n",
    "\n",
    "for index in range(len(similarity_matrix_dice)):\n",
    "    closest_id = np.argmax(similarity_matrix_dice[index])\n",
    "    print('the closest play to', document_names[index], '---is---', document_names[closest_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank_plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 16, 33, 27, 29, 14, 22, 17,  9, 32,  1, 10, 26, 11,  4,  6, 25,\n",
       "       31, 20, 30, 18, 28, 21,  5, 24,  3,  2, 35,  8, 34, 13, 12, 23, 15,\n",
       "        7, 19])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rank_plays(target_play_index, term_document_matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the plays to the target play.\n",
    "\n",
    "    Inputs:\n",
    "    target_play_index: The integer index of the play we want to compare all others against.\n",
    "    term_document_matrix: The term-document matrix as a mxn numpy array.\n",
    "    similarity_fn: Function that should be used to compared vectors for two\n",
    "      documents. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "\n",
    "    Returns:\n",
    "    A length-n list of integer indices corresponding to play names,\n",
    "    ordered by decreasing similarity to the play indexed by target_play_index\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    similarity = []\n",
    "    for index in range(term_document_matrix.shape[1]):\n",
    "        similarity.append([similarity_fn(get_column_vector(term_document_matrix, target_play_index), get_column_vector(term_document_matrix, index)), index])\n",
    "    \n",
    "    sorted_list = np.array(sorted(similarity, key=lambda x:x[0], reverse=True))\n",
    "    # print(sorted_list)\n",
    "    return sorted_list[:, 1].astype(int)\n",
    "\n",
    "rank_plays(0, term_document_matrix, compute_jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,   757, 19590, ..., 22599, 22600, 22601])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rank_words(target_word_index, matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the words to the target word.\n",
    "\n",
    "    Inputs:\n",
    "    target_word_index: The index of the word we want to compare all others against.\n",
    "    matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "    similarity_fn: Function that should be used to compared vectors for two word\n",
    "      ebeddings. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "\n",
    "    Returns:\n",
    "    A length-n list of integer word indices, ordered by decreasing similarity to the \n",
    "    target word indexed by word_index\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    similarity = []\n",
    "    for index in range(matrix.shape[0]):\n",
    "        similarity.append([similarity_fn(get_row_vector(matrix, target_word_index), get_row_vector(matrix, index)), index])\n",
    "    \n",
    "    sorted_list = np.array(sorted(similarity, key=lambda x:x[0], reverse=True))\n",
    "    # print(sorted_list)\n",
    "    return sorted_list[:, 1].astype(int)\n",
    "\n",
    "rank_words(0, term_context_matrix, compute_cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n",
      "Computing tf-idf matrix...\n",
      "Computing term context matrix...\n",
      "Computing PPMI matrix...\n",
      "\n",
      "The top most similar plays to \"Romeo and Juliet\" using compute_cosine_similarity are:\n",
      "1: Romeo and Juliet\n",
      "2: A Midsummer nights dream\n",
      "3: The Tempest\n",
      "\n",
      "The top most similar plays to \"Romeo and Juliet\" using compute_jaccard_similarity are:\n",
      "1: Romeo and Juliet\n",
      "2: Othello\n",
      "3: King Lear\n",
      "\n",
      "The top most similar plays to \"Romeo and Juliet\" using compute_dice_similarity are:\n",
      "1: Romeo and Juliet\n",
      "2: Othello\n",
      "3: King Lear\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on term-document frequency matrix are:\n",
      "1: juliet\n",
      "2: capulet\n",
      "3: pump\n",
      "4: laura\n",
      "5: pitcher\n",
      "6: behoveful\n",
      "7: hurdle\n",
      "8: capulets\n",
      "9: petrucio\n",
      "10: heartless\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on term-document frequency matrix are:\n",
      "1: juliet\n",
      "2: tybalt\n",
      "3: capulet\n",
      "4: nurse\n",
      "5: romeo\n",
      "6: mercutio\n",
      "7: friar\n",
      "8: montague\n",
      "9: laurence\n",
      "10: paris\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_dice_similarity on term-document frequency matrix are:\n",
      "1: juliet\n",
      "2: tybalt\n",
      "3: capulet\n",
      "4: nurse\n",
      "5: romeo\n",
      "6: mercutio\n",
      "7: friar\n",
      "8: montague\n",
      "9: laurence\n",
      "10: paris\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on term-context frequency matrix are:\n",
      "1: juliet\n",
      "2: warwick\n",
      "3: lucius\n",
      "4: gloucester\n",
      "5: antonio\n",
      "6: helena\n",
      "7: othello\n",
      "8: servants\n",
      "9: brutus\n",
      "10: claudio\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on term-context frequency matrix are:\n",
      "1: juliet\n",
      "2: silvia\n",
      "3: orlando\n",
      "4: proteus\n",
      "5: othello\n",
      "6: nurse\n",
      "7: demetrius\n",
      "8: hamlet\n",
      "9: iago\n",
      "10: leonato\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_dice_similarity on term-context frequency matrix are:\n",
      "1: juliet\n",
      "2: silvia\n",
      "3: orlando\n",
      "4: proteus\n",
      "5: othello\n",
      "6: nurse\n",
      "7: demetrius\n",
      "8: hamlet\n",
      "9: iago\n",
      "10: leonato\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on tf_idf matrix are:\n",
      "1: juliet\n",
      "2: procures\n",
      "3: benedicite\n",
      "4: ghostly\n",
      "5: lucio\n",
      "6: pump\n",
      "7: mab\n",
      "8: households\n",
      "9: blubbering\n",
      "10: stinted\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on tf_idf matrix are:\n",
      "1: juliet\n",
      "2: benvolio\n",
      "3: lucio\n",
      "4: mercutio\n",
      "5: capulet\n",
      "6: tybalt\n",
      "7: romeo\n",
      "8: capulets\n",
      "9: ghostly\n",
      "10: montagues\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_dice_similarity on tf_idf matrix are:\n",
      "1: juliet\n",
      "2: benvolio\n",
      "3: lucio\n",
      "4: mercutio\n",
      "5: capulet\n",
      "6: tybalt\n",
      "7: romeo\n",
      "8: capulets\n",
      "9: ghostly\n",
      "10: montagues\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on PPMI matrix are:\n",
      "1: juliet\n",
      "2: throe\n",
      "3: silvia\n",
      "4: benumbed\n",
      "5: sevenfold\n",
      "6: opposeless\n",
      "7: banditti\n",
      "8: executioners\n",
      "9: presenters\n",
      "10: mercade\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on PPMI matrix are:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tuples, document_names, vocab = read_in_shakespeare()\n",
    "\n",
    "    print('Computing term document matrix...')\n",
    "    td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "\n",
    "    print('Computing tf-idf matrix...')\n",
    "    tf_idf_matrix = create_tf_idf_matrix(td_matrix)\n",
    "\n",
    "    print('Computing term context matrix...')\n",
    "    tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "\n",
    "    print('Computing PPMI matrix...')\n",
    "    PPMI_matrix = create_PPMI_matrix(tc_matrix)\n",
    "\n",
    "    random_idx = random.randint(0, len(document_names)-1)\n",
    "    similarity_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe top most similar plays to \"%s\" using %s are:' % (document_names[random_idx], sim_fn.__qualname__))\n",
    "        ranks = rank_plays(random_idx, td_matrix, sim_fn)\n",
    "        for idx in range(0, 3):\n",
    "            doc_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, document_names[doc_id]))\n",
    "\n",
    "    word = 'juliet'\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe 10 most similar words to \"%s\" using %s on term-document frequency matrix are:' % (word, sim_fn.__qualname__))\n",
    "        ranks = rank_words(vocab_to_index[word], td_matrix, sim_fn)\n",
    "        for idx in range(0, 10):\n",
    "            word_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "\n",
    "    word = 'juliet'\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe 10 most similar words to \"%s\" using %s on term-context frequency matrix are:' % (word, sim_fn.__qualname__))\n",
    "        ranks = rank_words(vocab_to_index[word], tc_matrix, sim_fn)\n",
    "        for idx in range(0, 10):\n",
    "            word_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "\n",
    "    word = 'juliet'\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe 10 most similar words to \"%s\" using %s on tf_idf matrix are:' % (word, sim_fn.__qualname__))\n",
    "        ranks = rank_words(vocab_to_index[word], tf_idf_matrix, sim_fn)\n",
    "        for idx in range(0, 10):\n",
    "            word_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "\n",
    "    word = 'juliet'\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe 10 most similar words to \"%s\" using %s on PPMI matrix are:' % (word, sim_fn.__qualname__))\n",
    "        ranks = rank_words(vocab_to_index[word], PPMI_matrix, sim_fn)\n",
    "        for idx in range(0, 10):\n",
    "            word_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, vocab[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion part\n",
    "### cluster the plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "play_kmean = KMeans(n_clusters=3, random_state=0)\n",
    "plays = []\n",
    "for index in range(td_matrix.shape[1]):\n",
    "    plays.append(get_column_vector(td_matrix, index))\n",
    "\n",
    "play_kmean.fit(np.array(plays))\n",
    "\n",
    "three_types = {0:[], 1:[], 2:[]}\n",
    "for index in range(len(play_kmean.labels_)):\n",
    "    three_types[play_kmean.labels_[index]].append(document_names[index])\n",
    "\n",
    "three_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### term_character_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_line = []\n",
    "character_names = {}\n",
    "with open('will_play_text.csv') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=';')\n",
    "        for row in csv_reader:\n",
    "            character_name = row[4]\n",
    "            if len(character_name) > 0:\n",
    "                line = row[5]\n",
    "                line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
    "                line_tokens = [token.lower() for token in line_tokens]\n",
    "                character_names[character_name] = True\n",
    "\n",
    "                name_line.append((character_name, line_tokens))\n",
    "\n",
    "character_names = list(character_names.keys())\n",
    "term_character_matrix = create_term_document_matrix(name_line, character_names, vocab)\n",
    "\n",
    "print(term_character_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most/least similar characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "similarity_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
    "for sim_fn in similarity_fns:\n",
    "    print('using', sim_fn.__qualname__)\n",
    "    try:\n",
    "        similarity_matrix = sparse.load_npz(\"similarity_matrix_\" + sim_fn.__qualname__ + \".npz\").toarray()\n",
    "    except:\n",
    "        similarity_matrix = np.zeros((len(character_names), len(character_names)))\n",
    "        \n",
    "        for x in range(len(character_names)):\n",
    "            for y in range(x + 1, len(character_names)):\n",
    "                similarity_matrix[x][y] = similarity_matrix[y][x] = sim_fn(get_column_vector(term_character_matrix, x), get_column_vector(term_character_matrix, y))\n",
    "\n",
    "        sparse.save_npz(\"similarity_matrix_\" + sim_fn.__qualname__ + \".npz\", sparse.csc_matrix(similarity_matrix))\n",
    "    \n",
    "    print('the most similar character pairs (A, B) are:')\n",
    "    for index in range(len(similarity_matrix)):\n",
    "        closest_id = np.argmax(similarity_matrix[index])\n",
    "        print(character_names[index], '------', character_names[closest_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_kmean = KMeans(n_clusters=2, random_state=0)\n",
    "characters = []\n",
    "for index in range(term_character_matrix.shape[1]):\n",
    "    characters.append(get_column_vector(term_character_matrix, index))\n",
    "\n",
    "character_kmean.fit(np.array(plays))\n",
    "\n",
    "two_types = {0:[], 1:[]}\n",
    "for index in range(len(character_kmean.labels_)):\n",
    "    two_types[character_kmean.labels_[index]].append(character_names[index])\n",
    "\n",
    "two_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
