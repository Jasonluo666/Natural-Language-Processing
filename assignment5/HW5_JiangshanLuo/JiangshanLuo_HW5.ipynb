{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: English -> emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use 1-gram and 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='word')\n",
    "\n",
    "# Load the training data\n",
    "train_data = []\n",
    "with open(\"train/english_train.text\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "#         train_data.append(tknzr.tokenize(line.lower()))\n",
    "        train_data.append(line.lower())\n",
    "train_label = []\n",
    "with open(\"train/english_train.labels\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        train_label.append(int(line[:-1]))\n",
    "\n",
    "# Load the testing data\n",
    "test_data = []\n",
    "with open(\"test/english_test.text\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "#         test_data.append(tknzr.tokenize(line.lower()))\n",
    "        test_data.append(line.lower())\n",
    "test_label = []\n",
    "with open(\"test/english_test.labels\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        test_label.append(int(line[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize traning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'little': 257379,\n",
       " 'throwback': 445152,\n",
       " 'with': 496477,\n",
       " 'my': 297018,\n",
       " 'favourite': 151915,\n",
       " 'person': 340077,\n",
       " 'water': 483948,\n",
       " 'wall': 481147,\n",
       " 'little throwback': 258135,\n",
       " 'throwback with': 445240,\n",
       " 'with my': 497666,\n",
       " 'my favourite': 298080,\n",
       " 'favourite person': 151939,\n",
       " 'person water': 340207,\n",
       " 'water wall': 484050,\n",
       " 'glam': 179912,\n",
       " 'on': 323533,\n",
       " 'user': 467784,\n",
       " 'yesterday': 505711,\n",
       " 'for': 161572,\n",
       " 'kcon': 238775,\n",
       " 'makeup': 271308,\n",
       " 'using': 473534,\n",
       " 'in': 218978,\n",
       " 'featherette': 152378,\n",
       " 'glam on': 179928,\n",
       " 'on user': 324740,\n",
       " 'user yesterday': 473393,\n",
       " 'yesterday for': 505776,\n",
       " 'for kcon': 162535,\n",
       " 'kcon makeup': 238776,\n",
       " 'makeup using': 271404,\n",
       " 'using user': 473567,\n",
       " 'user in': 470206,\n",
       " 'in featherette': 219712,\n",
       " 'democracy': 120941,\n",
       " 'plaza': 346557,\n",
       " 'the': 429865,\n",
       " 'wake': 480708,\n",
       " 'of': 318234,\n",
       " 'stunning': 413325,\n",
       " 'outcome': 331282,\n",
       " 'decision2016': 119568,\n",
       " 'nbc': 303957,\n",
       " 'news': 307628,\n",
       " 'democracy plaza': 120947,\n",
       " 'plaza in': 346575,\n",
       " 'in the': 220985,\n",
       " 'the wake': 436067,\n",
       " 'wake of': 480714,\n",
       " 'of stunning': 320338,\n",
       " 'stunning outcome': 413369,\n",
       " 'outcome decision2016': 331283,\n",
       " 'decision2016 nbc': 119570,\n",
       " 'nbc news': 303961,\n",
       " 'then': 438578,\n",
       " 'amp': 22102,\n",
       " 'now': 314742,\n",
       " 'vilo': 478145,\n",
       " 'walt': 481394,\n",
       " 'disney': 124894,\n",
       " 'magic': 270039,\n",
       " 'kingdom': 241588,\n",
       " 'then amp': 438584,\n",
       " 'amp now': 23328,\n",
       " 'now vilo': 315264,\n",
       " 'vilo walt': 478146,\n",
       " 'walt disney': 481395,\n",
       " 'disney magic': 125000,\n",
       " 'magic kingdom': 270119,\n",
       " 'who': 492882,\n",
       " 'never': 305587,\n",
       " 'galaxy': 173461,\n",
       " 'far': 149872,\n",
       " 'away': 40645,\n",
       " 'who never': 493100,\n",
       " 'never galaxy': 305679,\n",
       " 'galaxy far': 173468,\n",
       " 'far far': 149898,\n",
       " 'far away': 149877,\n",
       " 'dinner': 123994,\n",
       " 'fla': 158173,\n",
       " 'tonight': 453107,\n",
       " 'pan': 333915,\n",
       " 'seared': 380488,\n",
       " 'salmon': 374882,\n",
       " 'over': 331735,\n",
       " 'couscous': 106961,\n",
       " 'veggie': 475706,\n",
       " 'salad': 374618,\n",
       " 'yum': 511705,\n",
       " 'florida': 159368,\n",
       " 'dinner in': 124101,\n",
       " 'in fla': 219733,\n",
       " 'fla tonight': 158175,\n",
       " 'tonight pan': 453501,\n",
       " 'pan seared': 333929,\n",
       " 'seared salmon': 380491,\n",
       " 'salmon over': 374895,\n",
       " 'over couscous': 331793,\n",
       " 'couscous veggie': 106963,\n",
       " 'veggie salad': 475716,\n",
       " 'salad yum': 374662,\n",
       " 'yum dinner': 511713,\n",
       " 'dinner florida': 124068,\n",
       " 'florida salmon': 159504,\n",
       " 'it': 227490,\n",
       " 'fav': 150860,\n",
       " 'seniors': 383163,\n",
       " 'last': 247497,\n",
       " 'game': 173707,\n",
       " 'congrats': 103301,\n",
       " 'beating': 51261,\n",
       " 'west': 489768,\n",
       " 'salem': 374762,\n",
       " 'it my': 228471,\n",
       " 'my fav': 298073,\n",
       " 'fav seniors': 150972,\n",
       " 'seniors last': 383172,\n",
       " 'last game': 247592,\n",
       " 'game congrats': 173748,\n",
       " 'congrats on': 103373,\n",
       " 'on beating': 323663,\n",
       " 'beating west': 51268,\n",
       " 'west west': 489940,\n",
       " 'west salem': 489906,\n",
       " 'got': 184987,\n",
       " 'to': 448362,\n",
       " 'go': 180809,\n",
       " 'formal': 164373,\n",
       " 'best': 56225,\n",
       " 'friend': 167630,\n",
       " 'phi': 340922,\n",
       " 'mu': 294898,\n",
       " 'at': 36031,\n",
       " 'jsu': 235470,\n",
       " 'got to': 185387,\n",
       " 'to to': 450472,\n",
       " 'to go': 449227,\n",
       " 'go formal': 180979,\n",
       " 'formal with': 164409,\n",
       " 'my best': 297334,\n",
       " 'best friend': 56465,\n",
       " 'friend phi': 167956,\n",
       " 'phi mu': 340946,\n",
       " 'mu at': 294899,\n",
       " 'at jsu': 37009,\n",
       " 'cause': 85899,\n",
       " 'miss': 286164,\n",
       " 'homies': 210378,\n",
       " 'cousinlove': 107079,\n",
       " 'indiana': 221739,\n",
       " 'university': 464757,\n",
       " 'cause miss': 85962,\n",
       " 'miss my': 286283,\n",
       " 'my little': 298755,\n",
       " 'little homies': 257734,\n",
       " 'homies throwback': 210408,\n",
       " 'throwback cousinlove': 445169,\n",
       " 'cousinlove indiana': 107081,\n",
       " 'indiana university': 221760,\n",
       " 'birthday': 60687,\n",
       " 'kisses': 242015,\n",
       " 'madison': 269691,\n",
       " 'wisconsin': 496116,\n",
       " 'birthday kisses': 61033,\n",
       " 'kisses madison': 242030,\n",
       " 'madison wisconsin': 269729,\n",
       " 'great': 187371,\n",
       " 'time': 446465,\n",
       " 'tuscaloosa': 461308,\n",
       " 'girl': 178071,\n",
       " 'bryant': 74347,\n",
       " 'denny': 121115,\n",
       " 'stadium': 406659,\n",
       " 'great time': 187837,\n",
       " 'time in': 446821,\n",
       " 'in tuscaloosa': 221073,\n",
       " 'tuscaloosa with': 461312,\n",
       " 'my girl': 298242,\n",
       " 'girl bryant': 178150,\n",
       " 'bryant denny': 74350,\n",
       " 'denny stadium': 121121,\n",
       " 'seguir': 382123,\n",
       " 'aprendiendo': 31544,\n",
       " 'del': 120358,\n",
       " 'mundo': 295871,\n",
       " 'de': 118714,\n",
       " 'las': 247332,\n",
       " 'berries': 56109,\n",
       " 'john': 234140,\n",
       " 'kennedy': 239507,\n",
       " 'international': 224005,\n",
       " 'airport': 15648,\n",
       " 'jfk': 233205,\n",
       " 'queens': 356854,\n",
       " 'ny': 316042,\n",
       " 'seguir aprendiendo': 382124,\n",
       " 'aprendiendo del': 31545,\n",
       " 'del mundo': 120369,\n",
       " 'mundo de': 295872,\n",
       " 'de las': 118758,\n",
       " 'las berries': 247337,\n",
       " 'berries john': 56115,\n",
       " 'john kennedy': 234183,\n",
       " 'kennedy international': 239513,\n",
       " 'international airport': 224008,\n",
       " 'airport jfk': 15693,\n",
       " 'jfk in': 233212,\n",
       " 'in queens': 220609,\n",
       " 'queens ny': 356877,\n",
       " 'ny ny': 316143,\n",
       " 'livinginparadise': 259192,\n",
       " 'ramada': 358566,\n",
       " 'beach': 49913,\n",
       " 'resort': 365379,\n",
       " 'livinginparadise ramada': 259193,\n",
       " 'ramada plaza': 358569,\n",
       " 'plaza beach': 346560,\n",
       " 'beach resort': 50285,\n",
       " 'order': 327986,\n",
       " 'your': 509838,\n",
       " 'life': 252647,\n",
       " 'be': 48794,\n",
       " 'you': 506993,\n",
       " 'must': 296763,\n",
       " 'first': 156567,\n",
       " 'learn': 249996,\n",
       " 'appreciate': 31416,\n",
       " 'in order': 220468,\n",
       " 'order for': 328003,\n",
       " 'for your': 163592,\n",
       " 'your life': 510448,\n",
       " 'life to': 253270,\n",
       " 'to be': 448575,\n",
       " 'be great': 49187,\n",
       " 'great you': 187897,\n",
       " 'you must': 508507,\n",
       " 'must first': 296778,\n",
       " 'first learn': 156818,\n",
       " 'learn to': 250022,\n",
       " 'to appreciate': 448510,\n",
       " 'appreciate it': 31425,\n",
       " 'it ny': 228534,\n",
       " 'these': 439547,\n",
       " 'girls': 178852,\n",
       " 'seniors2016': 383194,\n",
       " 'memories': 280658,\n",
       " 'werarab': 489387,\n",
       " 'loft': 260403,\n",
       " 'these girls': 439756,\n",
       " 'girls seniors2016': 179170,\n",
       " 'seniors2016 memories': 383195,\n",
       " 'memories werarab': 280775,\n",
       " 'werarab the': 489388,\n",
       " 'the loft': 433359,\n",
       " 'sully': 414802,\n",
       " 'has': 197655,\n",
       " 'his': 206857,\n",
       " 'own': 332251,\n",
       " 'style': 413492,\n",
       " 'yep': 505419,\n",
       " 'cowboy': 107484,\n",
       " 'boots': 67148,\n",
       " 'sully has': 414804,\n",
       " 'has his': 197771,\n",
       " 'his own': 207156,\n",
       " 'own beach': 332263,\n",
       " 'beach style': 50345,\n",
       " 'style yep': 413663,\n",
       " 'yep cowboy': 505425,\n",
       " 'cowboy boots': 107486,\n",
       " 'boots on': 67167,\n",
       " 'on the': 324656,\n",
       " 'the beach': 430428,\n",
       " 'cares': 83787,\n",
       " 'that': 428517,\n",
       " 'libra': 252448,\n",
       " 'season': 380539,\n",
       " 'still': 409713,\n",
       " 'celebrating': 86607,\n",
       " 'ti': 445733,\n",
       " 'tiday': 445886,\n",
       " 'who cares': 492931,\n",
       " 'cares that': 83796,\n",
       " 'that it': 429082,\n",
       " 'it libra': 228324,\n",
       " 'libra season': 252457,\n",
       " 'season still': 380724,\n",
       " 'still celebrating': 409764,\n",
       " 'celebrating ti': 86735,\n",
       " 'ti tiday': 445736,\n",
       " 'love': 263502,\n",
       " 'job': 233791,\n",
       " 'decorah': 119690,\n",
       " 'iowa': 224758,\n",
       " 'love my': 264517,\n",
       " 'my job': 298594,\n",
       " 'job decorah': 233828,\n",
       " 'decorah iowa': 119691,\n",
       " 'because': 52764,\n",
       " 'more': 291093,\n",
       " 'days': 117970,\n",
       " 'echo': 135804,\n",
       " 'because more': 52876,\n",
       " 'more days': 291196,\n",
       " 'days echo': 118064,\n",
       " 'echo beach': 135807,\n",
       " 'we': 485125,\n",
       " 'didn': 123112,\n",
       " 'stand': 407030,\n",
       " 'chance': 88118,\n",
       " 'all': 17037,\n",
       " 'coffeebeandnce': 99521,\n",
       " 'tower': 455664,\n",
       " 'records': 362077,\n",
       " 'we didn': 485293,\n",
       " 'didn stand': 123199,\n",
       " 'stand chance': 407035,\n",
       " 'chance at': 88124,\n",
       " 'at all': 36191,\n",
       " 'all user': 17804,\n",
       " 'user coffeebeandnce': 468880,\n",
       " 'coffeebeandnce tower': 99522,\n",
       " 'tower records': 455697,\n",
       " 'super': 417695,\n",
       " 'night': 309313,\n",
       " 'rosen': 371571,\n",
       " 'shingle': 387340,\n",
       " 'creek': 109004,\n",
       " 'hotel': 212130,\n",
       " 'convention': 104141,\n",
       " 'center': 87046,\n",
       " 'super night': 417771,\n",
       " 'night rosen': 310124,\n",
       " 'rosen shingle': 371572,\n",
       " 'shingle creek': 387341,\n",
       " 'creek hotel': 109058,\n",
       " 'hotel amp': 212136,\n",
       " 'amp convention': 22498,\n",
       " 'convention center': 104145,\n",
       " 'when': 491302,\n",
       " 'sun': 415705,\n",
       " 'hit': 207463,\n",
       " 'but': 76769,\n",
       " 'just': 236471,\n",
       " 'cute': 111773,\n",
       " 'when the': 491681,\n",
       " 'the sun': 435495,\n",
       " 'sun hit': 415813,\n",
       " 'hit you': 207547,\n",
       " 'you but': 507347,\n",
       " 'but you': 77342,\n",
       " 'you just': 508139,\n",
       " 'just still': 237077,\n",
       " 'still cute': 409783,\n",
       " 'acesnewyork': 11439,\n",
       " 'is': 225280,\n",
       " 'place': 345096,\n",
       " 'aces': 11434,\n",
       " 'new': 305991,\n",
       " 'york': 506657,\n",
       " 'acesnewyork is': 11440,\n",
       " 'is the': 226614,\n",
       " 'the place': 434341,\n",
       " 'place to': 345419,\n",
       " 'be tonight': 49780,\n",
       " 'tonight aces': 453127,\n",
       " 'aces new': 11437,\n",
       " 'new york': 307061,\n",
       " 'slamm': 394330,\n",
       " '1st': 2800,\n",
       " 'saturdays': 377370,\n",
       " 'wid': 493891,\n",
       " 'djkub': 126083,\n",
       " 'sweet': 419718,\n",
       " '221': 4542,\n",
       " 'lounge': 263390,\n",
       " 'slamm 1st': 394331,\n",
       " '1st saturdays': 2874,\n",
       " 'saturdays wid': 377382,\n",
       " 'wid djkub': 493892,\n",
       " 'djkub sweet': 126084,\n",
       " 'sweet 221': 419721,\n",
       " '221 lounge': 4543,\n",
       " 'haven': 199090,\n",
       " 'posted': 349697,\n",
       " 'awhile': 41154,\n",
       " 'should': 389245,\n",
       " 'probably': 352928,\n",
       " 'start': 407650,\n",
       " 'up': 465288,\n",
       " 'again': 14210,\n",
       " 'three': 444724,\n",
       " 'day': 116388,\n",
       " 'weekend': 487586,\n",
       " 'before': 54039,\n",
       " 'heading': 200293,\n",
       " 'haven posted': 199142,\n",
       " 'posted in': 349713,\n",
       " 'in awhile': 219162,\n",
       " 'awhile should': 41164,\n",
       " 'should probably': 389300,\n",
       " 'probably start': 352964,\n",
       " 'start up': 407768,\n",
       " 'up again': 465316,\n",
       " 'again three': 14524,\n",
       " 'three day': 444752,\n",
       " 'day weekend': 117701,\n",
       " 'weekend before': 487634,\n",
       " 'before heading': 54123,\n",
       " 'dad': 113018,\n",
       " 'named': 302264,\n",
       " 'our': 329344,\n",
       " 'cat': 85305,\n",
       " 'stoop': 410717,\n",
       " 'wanda': 481508,\n",
       " 'speaker': 403536,\n",
       " 'box': 68685,\n",
       " 'wtf': 502610,\n",
       " 'my dad': 297812,\n",
       " 'dad named': 113135,\n",
       " 'named our': 302285,\n",
       " 'our cat': 329539,\n",
       " 'cat stoop': 85368,\n",
       " 'stoop wanda': 410719,\n",
       " 'wanda speaker': 481512,\n",
       " 'speaker box': 403538,\n",
       " 'box wtf': 68748,\n",
       " 'nawwwll': 303888,\n",
       " 'he': 199715,\n",
       " 'said': 374263,\n",
       " 'she': 386016,\n",
       " 'belongs': 55483,\n",
       " 'fruit': 170415,\n",
       " 'loops': 262403,\n",
       " 'nawwwll he': 303889,\n",
       " 'he said': 200008,\n",
       " 'said she': 374375,\n",
       " 'she belongs': 386069,\n",
       " 'belongs on': 55485,\n",
       " 'on box': 323704,\n",
       " 'box of': 68717,\n",
       " 'of fruit': 319109,\n",
       " 'fruit loops': 170425,\n",
       " 'how': 213328,\n",
       " 'like': 254306,\n",
       " 'coffee': 99307,\n",
       " 'strong': 412432,\n",
       " 'will': 494473,\n",
       " 'wired': 496093,\n",
       " 'onlyused7creamers': 326530,\n",
       " 'cupofjoe': 111131,\n",
       " 'how like': 213454,\n",
       " 'like my': 254807,\n",
       " 'my coffee': 297697,\n",
       " 'coffee strong': 99479,\n",
       " 'strong will': 412507,\n",
       " 'will be': 494485,\n",
       " 'be wired': 49868,\n",
       " 'wired all': 496094,\n",
       " 'all day': 17205,\n",
       " 'day onlyused7creamers': 117286,\n",
       " 'onlyused7creamers strong': 326531,\n",
       " 'strong cupofjoe': 412448,\n",
       " 'disneyonice': 125292,\n",
       " 'daretodream': 115158,\n",
       " 'rapunzel': 359138,\n",
       " 'cross': 109761,\n",
       " 'insurance': 223770,\n",
       " 'arena': 32875,\n",
       " 'disneyonice daretodream': 125293,\n",
       " 'daretodream rapunzel': 115159,\n",
       " 'rapunzel cross': 359140,\n",
       " 'cross insurance': 109778,\n",
       " 'insurance arena': 223774,\n",
       " 'brunch': 73999,\n",
       " 'after': 13707,\n",
       " 'ballet': 45490,\n",
       " 'class': 96271,\n",
       " 'beautiful': 51513,\n",
       " 'saturday': 377027,\n",
       " 'dear': 119062,\n",
       " 'le': 249717,\n",
       " 'neuf': 305513,\n",
       " 'café': 79490,\n",
       " 'brunch after': 74003,\n",
       " 'after ballet': 13736,\n",
       " 'ballet class': 45493,\n",
       " 'class on': 96341,\n",
       " 'on beautiful': 323665,\n",
       " 'beautiful saturday': 52061,\n",
       " 'saturday with': 377313,\n",
       " 'my dear': 297860,\n",
       " 'dear friend': 119079,\n",
       " 'friend le': 167876,\n",
       " 'le neuf': 249743,\n",
       " 'neuf café': 305514,\n",
       " 'whiskey': 492409,\n",
       " 'kitchen': 242126,\n",
       " 'user whiskey': 473240,\n",
       " 'whiskey kitchen': 492422,\n",
       " 'newuscitizen': 307780,\n",
       " 'us': 466725,\n",
       " 'citizenship': 95275,\n",
       " 'and': 24777,\n",
       " 'immigration': 218542,\n",
       " 'services': 383946,\n",
       " 'newuscitizen us': 307783,\n",
       " 'us citizenship': 466823,\n",
       " 'citizenship and': 95276,\n",
       " 'and immigration': 26369,\n",
       " 'immigration services': 218544,\n",
       " 'so': 397615,\n",
       " 'much': 294961,\n",
       " 'fun': 171452,\n",
       " 'amazing': 20491,\n",
       " 'today': 450830,\n",
       " 'shooing': 388085,\n",
       " 'ssbeautyco': 406405,\n",
       " 'so much': 398003,\n",
       " 'much fun': 295150,\n",
       " 'fun with': 172036,\n",
       " 'with these': 498141,\n",
       " 'these amazing': 439556,\n",
       " 'amazing girls': 20696,\n",
       " 'girls today': 179229,\n",
       " 'today shooing': 451784,\n",
       " 'shooing for': 388086,\n",
       " 'for ssbeautyco': 163222,\n",
       " 'everything': 143880,\n",
       " 'about': 10460,\n",
       " 'this': 441799,\n",
       " 'hogvibes': 208098,\n",
       " 'fayetteville': 152031,\n",
       " 'arkansas': 33151,\n",
       " 'everything about': 143882,\n",
       " 'about this': 10726,\n",
       " 'this weekend': 443585,\n",
       " 'weekend hogvibes': 487813,\n",
       " 'hogvibes fayetteville': 208099,\n",
       " 'fayetteville arkansas': 152032,\n",
       " 'good': 183042,\n",
       " 'everyone': 143592,\n",
       " 'trenton': 458176,\n",
       " 'jersey': 232677,\n",
       " 'good night': 183385,\n",
       " 'night everyone': 309625,\n",
       " 'everyone trenton': 143826,\n",
       " 'trenton new': 458180,\n",
       " 'new jersey': 306491,\n",
       " 'miles': 284272,\n",
       " 'sunshine': 417425,\n",
       " 'smiles': 395889,\n",
       " 'palosverdes': 333820,\n",
       " 'california': 80006,\n",
       " 'holidays': 208608,\n",
       " 'rolling': 370514,\n",
       " 'hills': 206039,\n",
       " 'estates': 141621,\n",
       " 'miles sunshine': 284307,\n",
       " 'sunshine and': 417431,\n",
       " 'and smiles': 27670,\n",
       " 'smiles palosverdes': 395944,\n",
       " 'palosverdes you': 333822,\n",
       " 'you california': 507353,\n",
       " 'california holidays': 80102,\n",
       " 'holidays rolling': 208719,\n",
       " 'rolling hills': 370520,\n",
       " 'hills estates': 206070,\n",
       " 'back': 43228,\n",
       " 'north': 313071,\n",
       " 'carolina': 84215,\n",
       " 'wilmington': 494933,\n",
       " 'campus': 81416,\n",
       " 'trask': 457085,\n",
       " 'back at': 43257,\n",
       " 'at it': 36966,\n",
       " 'it university': 229008,\n",
       " 'university of': 464840,\n",
       " 'of north': 319766,\n",
       " 'north carolina': 313109,\n",
       " 'carolina wilmington': 84283,\n",
       " 'wilmington campus': 494937,\n",
       " 'campus trask': 81452,\n",
       " 'only': 326049,\n",
       " 'came': 80862,\n",
       " 'nachos': 301847,\n",
       " 'cheese': 89816,\n",
       " 'why': 493594,\n",
       " 'big': 58953,\n",
       " 'she only': 386302,\n",
       " 'only came': 326130,\n",
       " 'came for': 80875,\n",
       " 'for the': 163322,\n",
       " 'the nachos': 433835,\n",
       " 'nachos and': 301850,\n",
       " 'and cheese': 25383,\n",
       " 'cheese that': 89889,\n",
       " 'that why': 429674,\n",
       " 'why she': 493695,\n",
       " 'she my': 386286,\n",
       " 'my big': 297368,\n",
       " 'happy': 195185,\n",
       " 'madress': 269822,\n",
       " 'can': 81491,\n",
       " 'wait': 480526,\n",
       " 'led': 250392,\n",
       " 'zep': 512348,\n",
       " 'tomorrow': 452787,\n",
       " 'here': 203600,\n",
       " 'cool': 104492,\n",
       " 'pic': 342490,\n",
       " 'drinking': 131956,\n",
       " 'happy birthday': 195328,\n",
       " 'birthday madress': 61073,\n",
       " 'madress can': 269823,\n",
       " 'can wait': 81859,\n",
       " 'wait for': 480543,\n",
       " 'for led': 162590,\n",
       " 'led zep': 250399,\n",
       " 'zep tomorrow': 512349,\n",
       " 'tomorrow here': 452863,\n",
       " 'here cool': 203719,\n",
       " 'cool pic': 104663,\n",
       " 'pic of': 342607,\n",
       " 'of you': 320731,\n",
       " 'you drinking': 507632,\n",
       " 'backyard': 43926,\n",
       " 'one': 324954,\n",
       " 'favorite': 151164,\n",
       " 'places': 345480,\n",
       " 'nature': 303542,\n",
       " 'newengland': 307239,\n",
       " 'swampscott': 419336,\n",
       " 'massachussetts': 275537,\n",
       " 'the backyard': 430324,\n",
       " 'backyard one': 43949,\n",
       " 'one of': 325385,\n",
       " 'of my': 319701,\n",
       " 'my favorite': 298078,\n",
       " 'favorite places': 151595,\n",
       " 'places nature': 345506,\n",
       " 'nature newengland': 303608,\n",
       " 'newengland swampscott': 307244,\n",
       " 'swampscott massachussetts': 419337,\n",
       " 'a_plus25': 10119,\n",
       " 'shit': 387650,\n",
       " 'haters': 198192,\n",
       " 'produced': 353127,\n",
       " 'by': 77851,\n",
       " 'somh': 400653,\n",
       " 'link': 256249,\n",
       " 'full': 171134,\n",
       " 'song': 400831,\n",
       " 'bio': 60236,\n",
       " 'baton': 47789,\n",
       " 'a_plus25 shit': 10120,\n",
       " 'shit on': 387759,\n",
       " 'on my': 324282,\n",
       " 'my haters': 298390,\n",
       " 'haters produced': 198205,\n",
       " 'produced by': 353131,\n",
       " 'by user': 78718,\n",
       " 'user somh': 472348,\n",
       " 'somh link': 400654,\n",
       " 'link to': 256274,\n",
       " 'to full': 449186,\n",
       " 'full song': 171266,\n",
       " 'song in': 400887,\n",
       " 'in bio': 219234,\n",
       " 'bio baton': 60257,\n",
       " 'parts': 335818,\n",
       " 'get': 176072,\n",
       " 'play': 346026,\n",
       " 'things': 441203,\n",
       " 'pretty': 351684,\n",
       " 'weddingdress': 486857,\n",
       " 'of the': 320424,\n",
       " 'the best': 430501,\n",
       " 'best parts': 56668,\n",
       " 'parts of': 335826,\n",
       " 'job get': 233845,\n",
       " 'get to': 176434,\n",
       " 'to play': 449894,\n",
       " 'play with': 346172,\n",
       " 'with all': 496533,\n",
       " 'all things': 17753,\n",
       " 'things pretty': 441349,\n",
       " 'pretty weddingdress': 351940,\n",
       " 'missing': 286540,\n",
       " 'home': 209289,\n",
       " 'from': 169155,\n",
       " 'guy': 190706,\n",
       " 'hilton': 206207,\n",
       " 'head': 200128,\n",
       " 'island': 226989,\n",
       " 'south': 402437,\n",
       " 'missing my': 286605,\n",
       " 'my home': 298455,\n",
       " 'home away': 209329,\n",
       " 'away from': 40686,\n",
       " 'from home': 169583,\n",
       " 'home with': 209832,\n",
       " 'with this': 498147,\n",
       " 'this guy': 442542,\n",
       " 'guy hilton': 190844,\n",
       " 'hilton head': 206228,\n",
       " 'head island': 200183,\n",
       " 'island south': 227103,\n",
       " 'south carolina': 402468,\n",
       " 'been': 53397,\n",
       " 'four': 165055,\n",
       " 'years': 505017,\n",
       " 'll': 259345,\n",
       " 'forever': 163857,\n",
       " 'preforming': 350977,\n",
       " 'under': 463983,\n",
       " 'friday': 167118,\n",
       " 'lights': 254072,\n",
       " 'bo': 65126,\n",
       " 'rein': 363455,\n",
       " 'it been': 227662,\n",
       " 'been great': 53542,\n",
       " 'great four': 187541,\n",
       " 'four years': 165113,\n",
       " 'years amp': 505028,\n",
       " 'amp ll': 23134,\n",
       " 'll forever': 259403,\n",
       " 'forever miss': 163966,\n",
       " 'miss preforming': 286307,\n",
       " 'preforming under': 350978,\n",
       " 'under the': 464023,\n",
       " 'the friday': 432229,\n",
       " 'friday night': 167316,\n",
       " 'night lights': 309843,\n",
       " 'lights bo': 254087,\n",
       " 'bo rein': 65141,\n",
       " 'thankful': 427744,\n",
       " 'lovely': 265700,\n",
       " 'gem': 175401,\n",
       " 'believe': 55025,\n",
       " 'me': 277527,\n",
       " 'have': 198440,\n",
       " 'wonderful': 499345,\n",
       " 'people': 338634,\n",
       " 'surrounding': 418872,\n",
       " 'thankful for': 427766,\n",
       " 'for this': 163341,\n",
       " 'this lovely': 442792,\n",
       " 'lovely gem': 265755,\n",
       " 'gem believe': 175407,\n",
       " 'believe me': 55063,\n",
       " 'me you': 278991,\n",
       " 'you have': 507966,\n",
       " 'have wonderful': 199063,\n",
       " 'wonderful people': 499437,\n",
       " 'people surrounding': 339028,\n",
       " 'surrounding you': 418877,\n",
       " 'you you': 509448,\n",
       " 'was': 482377,\n",
       " 'say': 377963,\n",
       " 'hello': 202191,\n",
       " 'bae': 44380,\n",
       " 'ready': 360422,\n",
       " '2016': 3344,\n",
       " 'chapman': 88549,\n",
       " 'bmw': 65025,\n",
       " 'camelback': 80933,\n",
       " 'today was': 451980,\n",
       " 'was good': 482714,\n",
       " 'good day': 183149,\n",
       " 'day say': 117441,\n",
       " 'say hello': 378040,\n",
       " 'hello to': 202314,\n",
       " 'to the': 450436,\n",
       " 'the new': 433905,\n",
       " 'new bae': 306063,\n",
       " 'bae ready': 44529,\n",
       " 'ready for': 360469,\n",
       " 'for 2016': 161591,\n",
       " '2016 chapman': 3386,\n",
       " 'chapman bmw': 88550,\n",
       " 'bmw on': 65041,\n",
       " 'on camelback': 323735,\n",
       " 'asked': 35451,\n",
       " 'if': 216589,\n",
       " 'looked': 261960,\n",
       " 'ufo': 463187,\n",
       " 'san': 375644,\n",
       " 'antonio': 30292,\n",
       " 'river': 368366,\n",
       " 'walk': 480831,\n",
       " 'asked if': 35458,\n",
       " 'if this': 216717,\n",
       " 'this looked': 442781,\n",
       " 'looked like': 261979,\n",
       " 'like ufo': 255072,\n",
       " 'ufo the': 463189,\n",
       " 'the san': 434933,\n",
       " 'san antonio': 375647,\n",
       " 'antonio river': 30316,\n",
       " 'river walk': 368499,\n",
       " 'tustin': 461361,\n",
       " 'ca': 79002,\n",
       " 'at user': 37966,\n",
       " 'in tustin': 221076,\n",
       " 'tustin ca': 461362,\n",
       " 'tried': 458437,\n",
       " 'too': 453816,\n",
       " 'many': 273340,\n",
       " 'times': 447352,\n",
       " 'not': 313574,\n",
       " 'post': 349445,\n",
       " 'peosta': 339158,\n",
       " 'we tried': 485820,\n",
       " 'tried too': 458464,\n",
       " 'too many': 453987,\n",
       " 'many times': 273488,\n",
       " 'times to': 447477,\n",
       " 'to not': 449764,\n",
       " 'not post': 313937,\n",
       " 'post this': 349638,\n",
       " 'this peosta': 443020,\n",
       " 'peosta iowa': 339159,\n",
       " 'tis': 447981,\n",
       " 'jolly': 234626,\n",
       " 'treat': 457601,\n",
       " 'friends': 168157,\n",
       " 'family': 148284,\n",
       " 'or': 327443,\n",
       " 'loved': 265248,\n",
       " 'ones': 325785,\n",
       " 'holiday': 208328,\n",
       " 'tis the': 447984,\n",
       " 'the season': 435007,\n",
       " 'season to': 380735,\n",
       " 'be jolly': 49288,\n",
       " 'jolly treat': 234639,\n",
       " 'treat your': 457648,\n",
       " 'your friends': 510230,\n",
       " 'friends family': 168323,\n",
       " 'family or': 148818,\n",
       " 'or loved': 327637,\n",
       " 'loved ones': 265301,\n",
       " 'ones to': 325863,\n",
       " 'to one': 449795,\n",
       " 'of our': 319833,\n",
       " 'our holiday': 329862,\n",
       " 'thanks': 427898,\n",
       " 'making': 271511,\n",
       " 'sure': 418468,\n",
       " 'dance': 114359,\n",
       " 'alone': 18630,\n",
       " 'jr': 235386,\n",
       " 'murphy': 296037,\n",
       " 'thanks for': 427997,\n",
       " 'for making': 162664,\n",
       " 'making sure': 271646,\n",
       " 'sure never': 418530,\n",
       " 'never dance': 305631,\n",
       " 'dance alone': 114368,\n",
       " 'alone jr': 18654,\n",
       " 'jr murphy': 235407,\n",
       " 'eat': 135383,\n",
       " 'omg': 323328,\n",
       " 'peruvianfood': 340358,\n",
       " 'lunchtime': 268096,\n",
       " 'meeting': 279864,\n",
       " 'miami': 282680,\n",
       " 'ready to': 360568,\n",
       " 'to eat': 449022,\n",
       " 'eat omg': 135453,\n",
       " 'omg peruvianfood': 323396,\n",
       " 'peruvianfood lunchtime': 340359,\n",
       " 'lunchtime meeting': 268108,\n",
       " 'meeting birthday': 279873,\n",
       " 'birthday miami': 61094,\n",
       " 'miami florida': 282754,\n",
       " 'strides': 412235,\n",
       " 'jones': 234733,\n",
       " 'boardwalk': 65209,\n",
       " 'making strides': 271643,\n",
       " 'strides jones': 412237,\n",
       " 'jones beach': 234739,\n",
       " 'beach boardwalk': 49976,\n",
       " 'they': 440543,\n",
       " 'called': 80510,\n",
       " 'drama': 130932,\n",
       " 'queen': 356707,\n",
       " 'blew': 63414,\n",
       " 'them': 438031,\n",
       " 'kiss': 241942,\n",
       " 'western': 490043,\n",
       " 'connecticut': 103525,\n",
       " 'state': 408120,\n",
       " 'they called': 440579,\n",
       " 'called me': 80540,\n",
       " 'me drama': 277890,\n",
       " 'drama queen': 130939,\n",
       " 'queen so': 356787,\n",
       " 'so blew': 397678,\n",
       " 'blew them': 63420,\n",
       " 'them kiss': 438199,\n",
       " 'kiss western': 241990,\n",
       " 'western connecticut': 490052,\n",
       " 'connecticut state': 103530,\n",
       " 'state university': 408252,\n",
       " 'low': 266922,\n",
       " 'key': 239945,\n",
       " 'killin': 241061,\n",
       " 'dress': 131545,\n",
       " 'kouturekonnections': 243519,\n",
       " 'pittsburgh': 344787,\n",
       " 'pennsylvania': 338520,\n",
       " 'was low': 482849,\n",
       " 'low key': 266938,\n",
       " 'key killin': 239966,\n",
       " 'killin shit': 241066,\n",
       " 'shit tonight': 387798,\n",
       " 'tonight dress': 453267,\n",
       " 'dress kouturekonnections': 131594,\n",
       " 'kouturekonnections pittsburgh': 243520,\n",
       " 'pittsburgh pennsylvania': 344808,\n",
       " 'hey': 204593,\n",
       " 'doing': 127776,\n",
       " 'short': 388857,\n",
       " 'beat': 51123,\n",
       " 'set': 384123,\n",
       " 'around': 33423,\n",
       " '30': 5596,\n",
       " '00': 0,\n",
       " 're': 359808,\n",
       " 'area': 32757,\n",
       " 'hey all': 204598,\n",
       " 'all ll': 17458,\n",
       " 'll be': 259358,\n",
       " 'be doing': 49054,\n",
       " 'doing short': 127868,\n",
       " 'short beat': 388864,\n",
       " 'beat set': 51194,\n",
       " 'set around': 384130,\n",
       " 'around 30': 33430,\n",
       " '30 00': 5597,\n",
       " '00 all': 5,\n",
       " 'all if': 17390,\n",
       " 'if you': 216747,\n",
       " 'you re': 508793,\n",
       " 're in': 360021,\n",
       " 'the area': 430210,\n",
       " 'arbonne': 31790,\n",
       " 'glow': 180520,\n",
       " 'bronzer': 72851,\n",
       " 'houston': 213128,\n",
       " 'bun': 75774,\n",
       " 'texas': 426743,\n",
       " 'the arbonne': 430196,\n",
       " 'arbonne glow': 31791,\n",
       " 'glow love': 180549,\n",
       " 'love our': 264591,\n",
       " 'our bronzer': 329506,\n",
       " 'bronzer houston': 72853,\n",
       " 'houston makeup': 213183,\n",
       " 'makeup bun': 271323,\n",
       " 'bun houston': 75784,\n",
       " 'houston texas': 213220,\n",
       " 'crocheting': 109657,\n",
       " 'tv': 461414,\n",
       " 'lazy': 249530,\n",
       " 'egyptian': 136795,\n",
       " 'rose': 371444,\n",
       " 'queendom': 356832,\n",
       " 'chicago': 90711,\n",
       " 'il': 217367,\n",
       " 'crocheting and': 109659,\n",
       " 'and tv': 28136,\n",
       " 'tv lazy': 461440,\n",
       " 'lazy saturday': 249551,\n",
       " 'saturday egyptian': 377108,\n",
       " 'egyptian rose': 136797,\n",
       " 'rose queendom': 371481,\n",
       " 'queendom in': 356833,\n",
       " 'in chicago': 219409,\n",
       " 'chicago il': 90817,\n",
       " 'ran': 358664,\n",
       " 'into': 224262,\n",
       " 'backstage': 43826,\n",
       " 'throwbackthursday': 445254,\n",
       " '__________________________': 9698,\n",
       " 'that one': 429279,\n",
       " 'one time': 325565,\n",
       " 'time ran': 447033,\n",
       " 'ran into': 358675,\n",
       " 'into user': 224429,\n",
       " 'user backstage': 468216,\n",
       " 'backstage throwbackthursday': 43846,\n",
       " 'throwbackthursday __________________________': 445255,\n",
       " 'merry': 281584,\n",
       " 'christmas': 93083,\n",
       " 'westport': 490248,\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train_data)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<90000x513461 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1735710 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = vectorizer.transform(train_data)\n",
    "train_Y = train_label\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(dual=False, C=0.1, verbose=0)\n",
    "\n",
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 18,  2, ...,  3, 12,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = vectorizer.transform(test_data)\n",
    "test_Y = test_label\n",
    "\n",
    "y_pred = model.predict(test_X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F-Score (official): 26.549\n",
      "-----\n",
      "Micro F-Score: 32.567\n",
      "Precision: 32.567\n",
      "Recall: 32.567\n"
     ]
    }
   ],
   "source": [
    "with open(\"output/gold_labels_english.txt\", \"w\", encoding='utf8') as out:\n",
    "    for i, sent in enumerate(test_data): \n",
    "        gold = test_Y[i]\n",
    "        out.write(\"{}\\n\".format(gold))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "with open(\"output/predicted_labels_english.txt\", \"w\", encoding='utf8') as out:\n",
    "    for i, sent in enumerate(test_data): \n",
    "        pred = y_pred[i]\n",
    "        out.write(\"{}\\n\".format(pred))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "# python scorer_semeval18.py gold_labels_file predicted_labels_file\n",
    "import scorer_semeval18\n",
    "scorer_semeval18.main(\"output/gold_labels_english.txt\", \"output/predicted_labels_english.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Spanish -> emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Load the training data\n",
    "train_data = []\n",
    "with open(\"train/spanish_train.text\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "#         train_data.append(tknzr.tokenize(line.lower()))\n",
    "        train_data.append(line.lower())\n",
    "train_label = []\n",
    "with open(\"train/spanish_train.labels\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        train_label.append(int(line[:-1]))\n",
    "\n",
    "# Load the testing data\n",
    "test_data = []\n",
    "with open(\"test/spanish_test.text\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "#         test_data.append(tknzr.tokenize(line.lower()))\n",
    "        test_data.append(line.lower())\n",
    "test_label = []\n",
    "with open(\"test/spanish_test.labels\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        test_label.append(int(line[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<19000x126417 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 336610 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train_data)\n",
    "print(vectorizer.transform([train_data[0]]).toarray())\n",
    "\n",
    "train_X = vectorizer.transform(train_data)\n",
    "train_Y = train_label\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-model prediction (use LR finally which has higher performance finally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVM_model = LinearSVC(dual=False, C=0.1, verbose=0)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP_model = MLPClassifier(learning_rate_init=0.005, verbose=1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression_model = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_train(train_X, train_Y):\n",
    "    print('RandomForest_model')\n",
    "    RandomForest_model.fit(train_X, train_Y)\n",
    "\n",
    "    print('LogisticRegression_model')\n",
    "    LogisticRegression_model.fit(train_X, train_Y)\n",
    "\n",
    "    print('SVM_model')\n",
    "    SVM_model.fit(train_X, train_Y)\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    training_vec = []\n",
    "    training_vec.append(SVM_model.predict(train_X))\n",
    "    training_vec.append(RandomForest_model.predict(train_X))\n",
    "    training_vec.append(LogisticRegression_model.predict(train_X))\n",
    "\n",
    "    training_vec = np.array(training_vec)\n",
    "    print(training_vec.shape)\n",
    "    training_vec = np.rot90(training_vec)\n",
    "    print(training_vec.shape)\n",
    "\n",
    "    print('MLP_model')\n",
    "    MLP_model.fit(training_vec, train_Y)\n",
    "\n",
    "def multi_model_predict(X):\n",
    "    training_vec = []\n",
    "    training_vec.append(SVM_model.predict(X))\n",
    "    training_vec.append(RandomForest_model.predict(X))\n",
    "    training_vec.append(LogisticRegression_model.predict(X))\n",
    "\n",
    "    X_vec = np.array(training_vec)\n",
    "    print(X_vec.shape)\n",
    "    X_vec = np.rot90(training_vec)\n",
    "    print(X_vec.shape)\n",
    "    print(X_vec[:10], test_Y[:10])\n",
    "\n",
    "    return LogisticRegression_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest_model\n",
      "LogisticRegression_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_model\n",
      "(3, 19000)\n",
      "(19000, 3)\n",
      "MLP_model\n",
      "Iteration 1, loss = 2.72769438\n",
      "Iteration 2, loss = 2.65599988\n",
      "Iteration 3, loss = 2.65252733\n",
      "Iteration 4, loss = 2.64700458\n",
      "Iteration 5, loss = 2.64652764\n",
      "Iteration 6, loss = 2.64373563\n",
      "Iteration 7, loss = 2.64464399\n",
      "Iteration 8, loss = 2.64343260\n",
      "Iteration 9, loss = 2.64146340\n",
      "Iteration 10, loss = 2.64115478\n",
      "Iteration 11, loss = 2.64100623\n",
      "Iteration 12, loss = 2.64042587\n",
      "Iteration 13, loss = 2.64004212\n",
      "Iteration 14, loss = 2.64036430\n",
      "Iteration 15, loss = 2.63909404\n",
      "Iteration 16, loss = 2.63977563\n",
      "Iteration 17, loss = 2.63919987\n",
      "Iteration 18, loss = 2.63981908\n",
      "Iteration 19, loss = 2.63793526\n",
      "Iteration 20, loss = 2.63769332\n",
      "Iteration 21, loss = 2.63838221\n",
      "Iteration 22, loss = 2.63830372\n",
      "Iteration 23, loss = 2.63886352\n",
      "Iteration 24, loss = 2.63813942\n",
      "Iteration 25, loss = 2.63782758\n",
      "Iteration 26, loss = 2.63858911\n",
      "Iteration 27, loss = 2.63778886\n",
      "Iteration 28, loss = 2.63909443\n",
      "Iteration 29, loss = 2.63763086\n",
      "Iteration 30, loss = 2.63715409\n",
      "Iteration 31, loss = 2.63630767\n",
      "Iteration 32, loss = 2.63755513\n",
      "Iteration 33, loss = 2.63770988\n",
      "Iteration 34, loss = 2.63720509\n",
      "Iteration 35, loss = 2.63783479\n",
      "Iteration 36, loss = 2.63704072\n",
      "Iteration 37, loss = 2.63718080\n",
      "Iteration 38, loss = 2.63688547\n",
      "Iteration 39, loss = 2.63693955\n",
      "Iteration 40, loss = 2.63672808\n",
      "Iteration 41, loss = 2.63607477\n",
      "Iteration 42, loss = 2.63756244\n",
      "Iteration 43, loss = 2.63640071\n",
      "Iteration 44, loss = 2.63710825\n",
      "Iteration 45, loss = 2.63642093\n",
      "Iteration 46, loss = 2.63643217\n",
      "Iteration 47, loss = 2.63610638\n",
      "Iteration 48, loss = 2.63697831\n",
      "Iteration 49, loss = 2.63541788\n",
      "Iteration 50, loss = 2.63638448\n",
      "Iteration 51, loss = 2.63579217\n",
      "Iteration 52, loss = 2.63601351\n",
      "Iteration 53, loss = 2.63585170\n",
      "Iteration 54, loss = 2.63573055\n",
      "Iteration 55, loss = 2.63568723\n",
      "Iteration 56, loss = 2.63556948\n",
      "Iteration 57, loss = 2.63580068\n",
      "Iteration 58, loss = 2.63544200\n",
      "Iteration 59, loss = 2.63553169\n",
      "Iteration 60, loss = 2.63528180\n",
      "Iteration 61, loss = 2.63606173\n",
      "Iteration 62, loss = 2.63546475\n",
      "Iteration 63, loss = 2.63544000\n",
      "Iteration 64, loss = 2.63504831\n",
      "Iteration 65, loss = 2.63570388\n",
      "Iteration 66, loss = 2.63496619\n",
      "Iteration 67, loss = 2.63484283\n",
      "Iteration 68, loss = 2.63487541\n",
      "Iteration 69, loss = 2.63521329\n",
      "Iteration 70, loss = 2.63479755\n",
      "Iteration 71, loss = 2.63466643\n",
      "Iteration 72, loss = 2.63441564\n",
      "Iteration 73, loss = 2.63470466\n",
      "Iteration 74, loss = 2.63442405\n",
      "Iteration 75, loss = 2.63460933\n",
      "Iteration 76, loss = 2.63432741\n",
      "Iteration 77, loss = 2.63500611\n",
      "Iteration 78, loss = 2.63429356\n",
      "Iteration 79, loss = 2.63467398\n",
      "Iteration 80, loss = 2.63503680\n",
      "Iteration 81, loss = 2.63463392\n",
      "Iteration 82, loss = 2.63448348\n",
      "Iteration 83, loss = 2.63422168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "multi_model_train(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1000)\n",
      "(1000, 3)\n",
      "[[ 1  2  1]\n",
      " [ 1  0  1]\n",
      " [11 17 12]\n",
      " [ 1  0  0]\n",
      " [ 1  0  1]\n",
      " [15  0  0]\n",
      " [ 1  0  1]\n",
      " [ 0  0  0]\n",
      " [ 0  1  1]\n",
      " [ 1  2  1]] [2, 0, 3, 15, 3, 11, 18, 10, 0, 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  2,  2,  1,  0,  9, 14,  4, 15,  7,  0,  2,  2,  2,  0,\n",
       "        1,  1,  0,  5,  2,  2,  8,  1,  1,  5,  4,  0,  0,  0,  2,  1,  0,\n",
       "        1,  4,  2, 10,  2,  1,  0,  0,  2,  4,  2,  1,  1,  0,  7,  1,  7,\n",
       "        2,  0,  0,  0,  0,  1,  0,  2,  0,  0,  0,  0,  0,  1,  0,  1,  2,\n",
       "        1,  2,  2, 15,  0,  1,  2,  0,  1,  0,  2, 15,  0,  9,  2,  1,  1,\n",
       "        0,  0,  1,  4,  0,  1,  0,  0,  0,  1,  0,  2,  1,  0,  0,  0,  9,\n",
       "        0,  8,  1,  1,  0,  0,  1,  2,  2,  1,  1,  0,  1,  2,  0,  2,  0,\n",
       "        0,  2,  4,  2,  7,  0,  0,  0,  2,  0,  2,  2,  2,  0,  3,  0,  1,\n",
       "        0,  0,  2,  0,  2,  7,  2,  3,  0,  7,  2,  9,  2,  1,  0,  2,  2,\n",
       "        1, 10,  0,  1,  6,  1,  0,  0,  2,  0,  1,  4,  0,  0, 11,  1,  1,\n",
       "        0,  2,  0,  2,  0,  0,  1,  2,  1,  2,  2,  2,  0,  2, 10,  0,  0,\n",
       "        5,  0,  0,  2,  5,  1,  0,  0,  6,  2,  0,  0,  1,  2,  1,  2,  1,\n",
       "       14,  1,  2,  0,  0,  1,  2,  0,  0,  0,  5,  2,  0,  1,  0,  2, 17,\n",
       "        6,  2,  4,  0,  3,  0,  2,  6,  2,  0,  5,  0,  1,  2,  0,  5,  0,\n",
       "        5,  0,  1,  0,  7,  9,  0,  0,  2,  0,  0,  0,  0,  4,  1,  2,  0,\n",
       "        0,  2,  2,  0, 11,  0,  0,  2,  5,  0,  2,  1,  1, 12,  0,  0,  1,\n",
       "        9,  9,  4,  1,  2,  0, 13,  1,  0,  0,  0,  2,  1,  2,  2,  0,  6,\n",
       "        2,  1,  7,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  2,  7,  0,\n",
       "        1,  2,  2,  0,  2,  1,  3,  0,  2,  0,  1,  2,  2,  1,  2,  0,  2,\n",
       "        9, 15,  7,  0,  2,  1,  0,  0,  0,  2,  1,  2,  0,  2,  0,  2,  5,\n",
       "        0,  0,  1,  0,  2,  1,  2,  0,  4,  1, 15,  9,  2,  2,  2,  0,  1,\n",
       "        2,  0,  9,  5,  2,  2,  1,  1,  2,  4, 11,  1,  0,  0,  2,  2,  0,\n",
       "        1,  2,  0,  1,  0,  0,  6,  0,  0,  0,  0,  0,  2,  1,  0,  0,  4,\n",
       "        0,  1, 13,  9,  2,  0,  0,  0,  0,  2,  2,  0,  0,  0,  2,  2,  1,\n",
       "        0, 15,  5,  0,  2,  1,  2, 11,  0,  0,  2,  0,  0,  5,  2,  0,  0,\n",
       "        0,  2, 15,  0,  1,  0,  2,  0,  1,  2,  2,  0,  0,  1,  0,  0,  5,\n",
       "        7,  0,  3,  1,  2,  2,  0,  5,  1,  0,  9,  0, 16,  2,  2,  1,  1,\n",
       "        1,  0,  2,  9,  2,  1,  0,  1,  2,  4,  0,  0,  0,  1,  4,  9,  0,\n",
       "        0,  0,  0,  0,  1,  1,  0,  1,  0, 13,  0,  4,  0,  9,  0,  1,  0,\n",
       "        0,  0,  0,  2,  2,  0,  0,  0,  1,  0,  2,  2, 11,  0,  0,  2,  0,\n",
       "        0,  1,  1,  0,  0, 15,  2,  2,  2,  0,  1,  4,  0,  0,  2,  0,  1,\n",
       "        9,  2,  0,  1,  2,  2,  0,  0,  2,  0, 16,  0,  9,  0,  0,  0,  0,\n",
       "        1,  1,  1,  0,  5,  1,  2,  2,  0,  0,  1,  1,  9,  0,  0,  0,  1,\n",
       "        0,  0,  3,  2,  2,  0,  0,  9,  2,  0,  0,  4,  0,  0,  2,  2,  1,\n",
       "        0,  2,  2, 16,  0,  0,  1,  0,  4,  2,  2,  0,  0,  0,  2,  5,  1,\n",
       "        2,  0,  2,  2,  1,  0,  0, 10,  1,  2,  0,  2,  0,  2,  2,  5,  2,\n",
       "        9,  2,  0,  0, 10,  2,  0,  1,  2,  1,  1,  7,  0,  0,  0,  0,  5,\n",
       "        6,  0,  4,  0,  1,  7,  0,  0,  0,  0,  2,  0,  0,  2,  2,  2,  2,\n",
       "        1,  3,  0,  2,  0,  0,  1,  2,  6,  1,  1,  6,  2,  2,  0,  2,  2,\n",
       "        0,  1,  0,  0,  0, 10,  0,  1,  1,  0,  2,  4,  2,  1,  7,  0,  0,\n",
       "        2,  2,  1,  0,  0,  0,  0, 11,  0,  0,  4,  0,  0,  2,  0,  0,  1,\n",
       "       16,  2,  6,  0,  0,  0,  1,  1,  6,  1,  9,  2,  0,  8,  0,  2,  0,\n",
       "       16,  2,  2,  0,  0,  0,  0,  0,  0,  2,  2,  0,  4,  0,  2,  2,  0,\n",
       "        2,  0,  2,  6,  2,  0,  1,  1,  0,  2,  1,  2,  0,  0,  2, 12,  1,\n",
       "       15,  0,  1,  0,  7,  2,  0,  0,  0,  1,  5,  0,  0,  2, 18,  1,  5,\n",
       "        6,  1,  2, 15,  2,  0,  0,  6,  4,  0,  0,  0,  0,  2,  0,  1,  0,\n",
       "        2,  0,  1,  0,  1,  2,  1,  0,  1,  4,  0,  0,  1,  0,  4,  0,  0,\n",
       "       11,  2,  9,  0,  2,  0, 10, 18,  0,  2,  0,  0,  2,  0, 13,  1,  2,\n",
       "        1,  1, 14,  0,  0,  4,  0,  1,  0,  0,  1,  2, 15,  0,  0,  2,  0,\n",
       "        0,  0, 17,  0,  2,  0,  4,  0,  2,  5,  1,  2,  1,  4,  0,  2,  4,\n",
       "        0,  1, 13,  2,  0,  0,  2,  0,  0,  0,  0, 16,  2,  0,  9,  2,  2,\n",
       "        0,  0,  0,  2,  0,  0,  0,  0,  1,  1,  2,  2,  1,  0,  1,  0,  3,\n",
       "        2,  0,  0,  2, 10,  4,  9,  2,  0,  4,  1,  1,  0,  2,  0,  2,  2,\n",
       "        1,  0,  6,  0,  2,  1,  0,  5,  1,  0,  0,  0,  2,  1,  0,  0,  0,\n",
       "        2,  0,  3,  0,  0,  1, 15,  0, 11,  2,  0,  1,  0, 16,  2,  3,  2,\n",
       "        0,  3,  4, 10,  1,  7,  2,  8,  2,  2, 14,  0,  2,  1,  0,  2,  2,\n",
       "        0,  2,  0,  0,  0,  2, 10,  0,  1,  0,  0,  1,  0,  2,  0,  9,  2,\n",
       "        0,  9,  0,  0,  1,  0,  0,  1,  0,  2,  2,  2,  2,  4,  1,  1,  0,\n",
       "        1,  1,  0,  0,  1,  1,  0,  1,  0,  1,  0, 12,  1,  1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = vectorizer.transform(test_data)\n",
    "test_Y = test_label\n",
    "\n",
    "y_pred = multi_model_predict(test_X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F-Score (official): 18.467\n",
      "-----\n",
      "Micro F-Score: 32.567\n",
      "Precision: 32.567\n",
      "Recall: 32.567\n"
     ]
    }
   ],
   "source": [
    "with open(\"output/gold_labels_spanish.txt\", \"w\", encoding='utf8') as out:\n",
    "    for i, sent in enumerate(test_data): \n",
    "        gold = test_Y[i]\n",
    "        out.write(\"{}\\n\".format(gold))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "with open(\"output/predicted_labels_spanish.txt\", \"w\", encoding='utf8') as out:\n",
    "    for i, sent in enumerate(test_data): \n",
    "        pred = y_pred[i]\n",
    "        out.write(\"{}\\n\".format(pred))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "# python scorer_semeval18.py gold_labels_file predicted_labels_file\n",
    "import scorer_semeval18\n",
    "scorer_semeval18.main(\"output/gold_labels_spanish.txt\", \"output/predicted_labels_spanish.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: multilingual transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(to_lang=\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate English data into Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': '0',\n",
       " '1': '1',\n",
       " '2': '2',\n",
       " '3': '3',\n",
       " '5': '4',\n",
       " '6': '10',\n",
       " '7': '15',\n",
       " '8': '11',\n",
       " '9': '5',\n",
       " '13': '12',\n",
       " '14': '7',\n",
       " '16': '18',\n",
       " '19': '13'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hashmap = {}\n",
    "with open(\"mapping/english_mapping.txt\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        en_hashmap[line[0]] = line[1]\n",
    "\n",
    "es_hashmap = {}\n",
    "with open(\"mapping/spanish_mapping.txt\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        es_hashmap[line[1]] = line[0]\n",
    "\n",
    "en2es_hashmap = {}\n",
    "for key in en_hashmap:\n",
    "    if en_hashmap[key] in es_hashmap:\n",
    "        en2es_hashmap[key] = es_hashmap[en_hashmap[key]]\n",
    "en2es_hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer='word')\n",
    "\n",
    "# Load the training data\n",
    "train_data = []\n",
    "with open(\"train/spanish_train.text\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "#         train_data.append(tknzr.tokenize(line.lower()))\n",
    "        train_data.append(line.lower())\n",
    "train_label = []\n",
    "with open(\"train/spanish_train.labels\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        train_label.append(int(line[:-1]))\n",
    "\n",
    "# load extra data\n",
    "extra_label = []\n",
    "missing_pos = []\n",
    "with open(\"train/english_train.labels\", 'r', encoding='utf8') as f:\n",
    "    for index, line in enumerate(f.readlines()[:-10000]):\n",
    "        if line[:-1] in en2es_hashmap:\n",
    "            extra_label.append(int(line[:-1]))\n",
    "        else:\n",
    "            missing_pos.append(index)\n",
    "\n",
    "extra_data = []\n",
    "with open(\"train/extra_data.txt\", 'r', encoding='utf8') as f:\n",
    "    for index, line in enumerate(f.readlines()):\n",
    "        if index not in missing_pos:\n",
    "            extra_data.append(line.lower())\n",
    "\n",
    "train_data += extra_data\n",
    "train_label += extra_label\n",
    "\n",
    "# Load the testing data\n",
    "test_data = []\n",
    "with open(\"test/spanish_test.text\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "#         test_data.append(tknzr.tokenize(line.lower()))\n",
    "        test_data.append(line.lower())\n",
    "test_label = []\n",
    "with open(\"test/spanish_test.labels\", 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        test_label.append(int(line[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<79941x399707 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 736136 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train_data)\n",
    "print(vectorizer.transform([train_data[0]]).toarray())\n",
    "\n",
    "train_X = vectorizer.transform(train_data)\n",
    "train_Y = train_label\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression_model = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "\n",
    "LogisticRegression_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  2,  2,  0,  0,  9, 14,  4,  0,  2,  0,  2,  2,  2,  0,\n",
       "        0,  1,  4,  5,  0,  0,  0,  0,  0,  5,  1,  0,  0,  0,  1,  1,  1,\n",
       "        1,  4,  0,  1,  0,  1,  0,  0,  2,  1,  2,  2,  0,  0,  7,  0,  2,\n",
       "        2,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  1,  0,  0,  0,\n",
       "        1,  0,  1,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  9,  2,  1,  1,\n",
       "        0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  2,  1,  0,  0,  0,  9,\n",
       "        0,  0,  1,  1,  0,  5,  0,  2,  2,  1,  1,  0,  1,  0,  0,  0,  0,\n",
       "        0,  2,  0,  2,  0,  0,  0,  2,  2,  0,  2,  2,  2,  0,  3,  0,  1,\n",
       "        0,  1,  2,  0,  1,  3,  2,  0,  0,  0,  2,  0,  1,  0,  1,  0,  2,\n",
       "        0,  0,  0,  1,  0,  0,  0,  0,  2,  0,  1,  3,  0,  0,  0,  1,  0,\n",
       "        0,  0,  0,  2,  0,  0,  0,  2,  1,  0,  2,  2,  0,  0, 10,  0,  0,\n",
       "        5,  0,  0,  2,  5,  1,  0,  0,  0,  1,  0,  0,  0,  2,  0,  2,  2,\n",
       "       14,  1,  0,  0,  0,  0,  0,  0,  4,  0,  5,  2,  0,  1,  0,  0,  0,\n",
       "        1,  2,  0,  0,  0,  0,  2,  0,  2,  0,  0,  0,  1,  1,  0,  5,  0,\n",
       "        1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  4,  2,  0,  2,  0,\n",
       "        0,  2,  5,  0,  0,  0,  2,  0,  1,  0,  2,  0,  0, 12, 13,  0,  0,\n",
       "        0,  9,  1,  1,  0,  2,  1,  0,  0,  0,  0,  2,  1,  0,  0,  0,  0,\n",
       "        2,  1,  7,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  7,  0,\n",
       "        0,  0,  1,  0,  0,  2,  0,  0,  2,  0,  1,  2,  2,  1,  2,  0,  0,\n",
       "        9, 15,  7,  0,  0,  0,  0,  0,  0,  0,  1,  2,  0,  2,  0,  0,  5,\n",
       "        0,  0,  1,  0,  0,  0,  2,  0,  1,  1,  0,  9,  0,  2,  2,  0,  0,\n",
       "        2,  0,  0,  5,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  2,  2,  0,\n",
       "        0,  2,  1,  0,  0,  0,  6,  0,  0,  0,  0,  0,  2,  0,  0,  2,  0,\n",
       "        0,  1,  0,  9,  2,  0,  1,  0,  0,  2,  2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  4,  0,  2,  1,  0,  1,  0,  0,  2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  2,  0,  0,  0,  0,  2,  1,  0,  2,  0,  0,  0,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  2,  0,  0,  1,  0,  0,  0,  2,  1,  0,  0,  0,\n",
       "        0,  0,  2,  0,  2,  0,  0,  1,  1,  4,  0,  0,  0,  1,  0,  9,  0,\n",
       "        2,  0,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n",
       "        0,  0,  0,  2,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  1,  0,\n",
       "        0,  0,  2,  0,  0, 15,  0,  2,  2,  0,  0,  8, 12,  0,  0,  0,  0,\n",
       "        0,  2,  0,  0,  2,  1,  0,  0,  1,  0, 16,  0,  0,  1,  0,  0,  0,\n",
       "        0,  1,  1,  1,  0,  1,  2,  2,  0,  0,  0,  0,  9,  0,  0,  0,  1,\n",
       "        0,  0,  3,  0,  1,  0,  2,  9,  0,  0,  0,  4,  0,  0,  2,  0,  0,\n",
       "        0,  0,  0, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
       "        2,  0,  2,  2,  0,  0,  0,  1,  0,  0,  0,  2,  1,  0,  2,  0,  2,\n",
       "        0,  0,  0,  2,  3,  0,  0,  1,  1,  2,  1,  0,  0,  0,  1,  2,  0,\n",
       "        0,  0,  1,  0,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,\n",
       "        0,  2,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  1,  2,  0,\n",
       "        0,  0,  0,  0,  0, 10,  0,  1,  2,  0,  0,  1,  0,  0,  2,  0,  0,\n",
       "        0,  0,  1,  0,  0,  0,  0,  0,  1,  0, 18,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  1,  6,  0,  1,  2,  0,  9,  0,  2,  0,\n",
       "       16,  2,  1,  0,  0,  0,  0,  0,  2,  2,  0,  0,  1, 12,  2,  2,  0,\n",
       "        0,  0,  1,  0,  1, 11,  1,  1,  0,  2,  1,  2,  1,  0,  2,  0,  0,\n",
       "        0,  0,  0,  0,  1,  2,  0,  0,  0,  0,  5,  0,  0,  2,  0,  0,  5,\n",
       "        0,  1,  2, 15,  2,  0,  0,  6,  1,  0,  0,  0,  0,  0,  0,  1,  0,\n",
       "        0,  0,  1,  0,  1,  0,  1,  0,  1,  4,  0,  0,  0,  0,  4,  0,  0,\n",
       "        1,  1,  9,  0,  1,  0,  9,  0,  0,  1,  0,  0,  0,  0,  1,  1,  0,\n",
       "        0,  1,  1,  0,  0,  4,  0,  1,  0,  0,  1,  0,  0,  0,  0,  2,  0,\n",
       "        0,  0, 17,  0,  0,  0,  2,  0,  2,  5,  0,  1,  0, 18,  0,  0,  4,\n",
       "        0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  0,  0,  0,  2,\n",
       "        0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  2,  1,  0,  1,  0,  0,\n",
       "        2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  2,  0,  1,  0,\n",
       "        0,  0,  0,  0,  2,  0,  0,  2,  0,  0,  1,  0,  2,  1,  0,  0,  0,\n",
       "        2,  0,  3,  0,  0,  0,  0,  0,  0,  2,  0,  0,  9,  0, 17,  3,  2,\n",
       "        0,  0,  1, 10,  1,  8,  1,  1,  7,  2,  1,  0,  0,  1,  2,  2,  0,\n",
       "        0,  2,  0,  0,  0,  2, 13,  0,  0,  0,  0,  1,  0,  0,  0,  9,  2,\n",
       "        0,  2,  0,  0,  0,  0,  0,  0,  0,  2,  0,  2,  2,  4,  0,  1,  0,\n",
       "        1,  1,  0,  2,  0,  0,  0,  0,  0,  2,  0,  0,  1,  0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = vectorizer.transform(test_data)\n",
    "test_Y = test_label\n",
    "\n",
    "y_pred = LogisticRegression_model.predict(test_X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F-Score (official): 14.888\n",
      "-----\n",
      "Micro F-Score: 27.672\n",
      "Precision: 27.672\n",
      "Recall: 27.672\n"
     ]
    }
   ],
   "source": [
    "with open(\"output/gold_labels_spanish_plus.txt\", \"w\", encoding='utf8') as out:\n",
    "    for i, sent in enumerate(test_data): \n",
    "        gold = test_Y[i]\n",
    "        out.write(\"{}\\n\".format(gold))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "with open(\"output/predicted_labels_spanish_plus.txt\", \"w\", encoding='utf8') as out:\n",
    "    for i, sent in enumerate(test_data): \n",
    "        pred = y_pred[i]\n",
    "        out.write(\"{}\\n\".format(pred))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "# python scorer_semeval18.py gold_labels_file predicted_labels_file\n",
    "import scorer_semeval18\n",
    "scorer_semeval18.main(\"output/gold_labels_spanish_plus.txt\", \"output/predicted_labels_spanish_plus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
